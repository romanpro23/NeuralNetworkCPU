//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32415258
// Cuda compilation tools, release 12.1, V12.1.66
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	fill
.global .align 4 .u32 SharedMemorySize = 32768;
.global .align 4 .u32 BLOCK_DIM = 32;
.const .align 2 .b8 sh[28];
// _ZZ12MatMulKernelE8blockElt has been demoted
// _ZZ12MatMulKernelE9blockxInd has been demoted
// _ZZ12MatMulKernelE9blockyInd has been demoted
// _ZZ12MatMulKernelE1b has been demoted
// _ZZ13MatMulKernelTE8blockElt has been demoted
// _ZZ13MatMulKernelTE9blockxInd has been demoted
// _ZZ13MatMulKernelTE9blockyInd has been demoted
// _ZZ13MatMulKernelTE1b has been demoted
// _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax has been demoted
// _ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx has been demoted
// _ZZ19sharedMem_transposeE8M_Shared has been demoted
// _ZZ24sharedMem_transpose_halfE8M_Shared has been demoted
// _ZZ33matrixTransposeSolveBankConflictsE3mat has been demoted
// _ZZ11transposeV3E3s_A has been demoted
.global .align 4 .u32 BLOCK_SIZE = 32;
// _ZZ13matvec_kernelE8x_shared has been demoted
// _ZZ18matvec_kernel_halfE8x_shared has been demoted
// _ZZ7SoftmaxE3max has been demoted
// _ZZ7SoftmaxE3sum has been demoted
// _ZZ12Softmax_halfE3max has been demoted
// _ZZ12Softmax_halfE3sum has been demoted

.visible .entry fill(
	.param .u64 fill_param_0,
	.param .align 2 .b8 fill_param_1[2],
	.param .u32 fill_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs1, [fill_param_1];
	ld.param.u64 	%rd1, [fill_param_0];
	ld.param.u32 	%r2, [fill_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.u16 	[%rd4], %rs1;

$L__BB0_2:
	ret;

}
	// .globl	fill_float
.visible .entry fill_float(
	.param .u64 fill_float_param_0,
	.param .f32 fill_float_param_1,
	.param .u32 fill_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [fill_float_param_0];
	ld.param.f32 	%f1, [fill_float_param_1];
	ld.param.u32 	%r2, [fill_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.f32 	[%rd4], %f1;

$L__BB1_2:
	ret;

}
	// .globl	float2HalfVector
.visible .entry float2HalfVector(
	.param .u64 float2HalfVector_param_0,
	.param .u64 float2HalfVector_param_1,
	.param .u32 float2HalfVector_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<10>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [float2HalfVector_param_0];
	ld.param.u64 	%rd3, [float2HalfVector_param_1];
	ld.param.u32 	%r2, [float2HalfVector_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_6;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f1;}

	// end inline asm
	setp.eq.s16 	%p2, %rs9, 31744;
	@%p2 bra 	$L__BB2_4;
	bra.uni 	$L__BB2_2;

$L__BB2_4:
	ld.const.u16 	%rs9, [sh+24];
	bra.uni 	$L__BB2_5;

$L__BB2_2:
	setp.ne.s16 	%p3, %rs9, -1024;
	@%p3 bra 	$L__BB2_5;

	ld.const.u16 	%rs8, [sh+24];
	// begin inline asm
	{neg.f16 %rs9,%rs8;
}
	// end inline asm

$L__BB2_5:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs9;

$L__BB2_6:
	ret;

}
	// .globl	half2FloatVector
.visible .entry half2FloatVector(
	.param .u64 half2FloatVector_param_0,
	.param .u64 half2FloatVector_param_1,
	.param .u32 half2FloatVector_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [half2FloatVector_param_0];
	ld.param.u64 	%rd2, [half2FloatVector_param_1];
	ld.param.u32 	%r2, [half2FloatVector_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB3_2:
	ret;

}
	// .globl	gelu
.visible .entry gelu(
	.param .u64 gelu_param_0,
	.param .u64 gelu_param_1,
	.param .u32 gelu_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [gelu_param_0];
	ld.param.u64 	%rd3, [gelu_param_1];
	ld.param.u32 	%r2, [gelu_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f7, %f1, %f1;
	mul.ftz.f32 	%f8, %f1, %f7;
	mul.ftz.f32 	%f9, %f8, 0f3D122277;
	fma.rn.ftz.f32 	%f2, %f1, 0f3F4C422A, %f9;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;

$L__BB4_3:
	mul.ftz.f32 	%f18, %f2, %f2;
	mov.f32 	%f19, 0fBD563CAE;
	mov.f32 	%f20, 0f3C80F082;
	fma.rn.ftz.f32 	%f21, %f20, %f18, %f19;
	mov.f32 	%f22, 0f3E085941;
	fma.rn.ftz.f32 	%f23, %f21, %f18, %f22;
	mov.f32 	%f24, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f25, %f23, %f18, %f24;
	mov.f32 	%f26, 0f00000000;
	fma.rn.ftz.f32 	%f27, %f25, %f18, %f26;
	fma.rn.ftz.f32 	%f31, %f27, %f2, %f2;
	bra.uni 	$L__BB4_4;

$L__BB4_2:
	mul.ftz.f32 	%f10, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f11, %f10;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f3F800000;
	rcp.approx.ftz.f32 	%f14, %f12;
	mov.f32 	%f15, 0fC0000000;
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f17, 0f3F800000, %f16, %p3;
	mov.b32 	%r6, %f17;
	mov.b32 	%r7, %f2;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f31, %r9;

$L__BB4_4:
	add.ftz.f32 	%f28, %f31, 0f3F800000;
	mul.ftz.f32 	%f29, %f1, 0f3F000000;
	mul.ftz.f32 	%f30, %f29, %f28;
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f30;

$L__BB4_5:
	ret;

}
	// .globl	gelu_half
.visible .entry gelu_half(
	.param .u64 gelu_half_param_0,
	.param .u64 gelu_half_param_1,
	.param .u32 gelu_half_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<27>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [gelu_half_param_0];
	ld.param.u64 	%rd2, [gelu_half_param_1];
	ld.param.u32 	%r2, [gelu_half_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs3, [%rd5];
	ld.const.u16 	%rs2, [sh+2];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs4,%rs3,%rs3;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs7,%rs4,%rs3;
}
	// end inline asm
	ld.const.u16 	%rs11, [sh+4];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs7;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs1,%rs10;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs13;}

	// end inline asm
	sin.approx.ftz.f32 	%f3, %f1;
	cos.approx.ftz.f32 	%f4, %f1;
	div.approx.ftz.f32 	%f2, %f3, %f4;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs17, %f2;}

	// end inline asm
	ld.const.u16 	%rs19, [sh+6];
	// begin inline asm
	{mul.f16 %rs18,%rs19,%rs3;
}
	// end inline asm
	ld.const.u16 	%rs22, [sh+8];
	// begin inline asm
	{add.f16 %rs21,%rs22,%rs17;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs24,%rs18,%rs21;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs24;

$L__BB5_2:
	ret;

}
	// .globl	MatAdd
.visible .entry MatAdd(
	.param .u64 MatAdd_param_0,
	.param .u64 MatAdd_param_1,
	.param .u32 MatAdd_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [MatAdd_param_0];
	ld.param.u64 	%rd2, [MatAdd_param_1];
	ld.param.u32 	%r2, [MatAdd_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB6_2:
	ret;

}
	// .globl	MatAdd_half
.visible .entry MatAdd_half(
	.param .u64 MatAdd_half_param_0,
	.param .u64 MatAdd_half_param_1,
	.param .u32 MatAdd_half_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [MatAdd_half_param_0];
	ld.param.u64 	%rd3, [MatAdd_half_param_1];
	ld.param.u32 	%r2, [MatAdd_half_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB7_6;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.u16 	%rs7, [%rd1];
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs8, [%rd7];
	// begin inline asm
	{add.f16 %rs11,%rs7,%rs8;
}
	// end inline asm
	st.global.u16 	[%rd1], %rs11;
	setp.eq.s16 	%p2, %rs11, 31744;
	@%p2 bra 	$L__BB7_4;
	bra.uni 	$L__BB7_2;

$L__BB7_4:
	ld.const.u16 	%rs11, [sh+24];
	bra.uni 	$L__BB7_5;

$L__BB7_2:
	setp.ne.s16 	%p3, %rs11, -1024;
	@%p3 bra 	$L__BB7_5;

	ld.const.u16 	%rs10, [sh+24];
	// begin inline asm
	{neg.f16 %rs11,%rs10;
}
	// end inline asm

$L__BB7_5:
	st.global.u16 	[%rd1], %rs11;

$L__BB7_6:
	ret;

}
	// .globl	imageVector
.visible .entry imageVector(
	.param .u64 imageVector_param_0,
	.param .u64 imageVector_param_1,
	.param .u32 imageVector_param_2,
	.param .u32 imageVector_param_3,
	.param .u32 imageVector_param_4,
	.param .u32 imageVector_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [imageVector_param_0];
	ld.param.u64 	%rd14, [imageVector_param_1];
	ld.param.u32 	%r29, [imageVector_param_2];
	ld.param.u32 	%r26, [imageVector_param_3];
	ld.param.u32 	%r27, [imageVector_param_4];
	ld.param.u32 	%r28, [imageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r33, %r31, %r30, %r32;
	mul.lo.s32 	%r1, %r33, %r28;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r37, %r35, %r34, %r36;
	mul.lo.s32 	%r2, %r37, %r28;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %ntid.z;
	mov.u32 	%r40, %tid.z;
	mad.lo.s32 	%r3, %r39, %r38, %r40;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r28;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB8_12;

	setp.lt.s32 	%p6, %r28, 1;
	@%p6 bra 	$L__BB8_12;

	mul.lo.s32 	%r41, %r28, %r27;
	mul.lo.s32 	%r42, %r41, %r28;
	add.s32 	%r43, %r1, %r3;
	mad.lo.s32 	%r4, %r43, %r26, %r2;
	setp.lt.s32 	%p7, %r27, 1;
	div.s32 	%r44, %r1, %r28;
	mul.lo.s32 	%r45, %r42, %r26;
	div.s32 	%r46, %r45, %r28;
	div.s32 	%r47, %r2, %r28;
	mul.lo.s32 	%r48, %r41, %r3;
	mad.lo.s32 	%r49, %r47, %r42, %r48;
	mad.lo.s32 	%r61, %r44, %r46, %r49;
	@%p7 bra 	$L__BB8_12;

	add.s32 	%r6, %r27, -1;
	and.b32  	%r7, %r27, 3;
	sub.s32 	%r8, %r27, %r7;
	add.s64 	%rd3, %rd1, 4;
	add.s64 	%rd4, %rd2, 4;
	mov.u32 	%r53, 0;

$L__BB8_4:
	add.s32 	%r52, %r4, %r53;
	mul.lo.s32 	%r59, %r52, %r27;
	setp.lt.u32 	%p8, %r6, 3;
	@%p8 bra 	$L__BB8_7;

	mul.wide.s32 	%rd15, %r61, 2;
	add.s64 	%rd20, %rd3, %rd15;
	mul.wide.s32 	%rd16, %r59, 2;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r57, %r8;

$L__BB8_6:
	ld.global.nc.u16 	%rs1, [%rd19+-4];
	st.global.u16 	[%rd20+-4], %rs1;
	ld.global.nc.u16 	%rs2, [%rd19+-2];
	st.global.u16 	[%rd20+-2], %rs2;
	ld.global.nc.u16 	%rs3, [%rd19];
	st.global.u16 	[%rd20], %rs3;
	ld.global.nc.u16 	%rs4, [%rd19+2];
	st.global.u16 	[%rd20+2], %rs4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p9, %r57, 0;
	@%p9 bra 	$L__BB8_6;

$L__BB8_7:
	mov.u32 	%r20, %r61;
	setp.eq.s32 	%p10, %r7, 0;
	@%p10 bra 	$L__BB8_11;

	setp.eq.s32 	%p11, %r7, 1;
	mul.wide.s32 	%rd17, %r59, 2;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.u16 	%rs5, [%rd11];
	mul.wide.s32 	%rd18, %r20, 2;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.u16 	[%rd12], %rs5;
	add.s32 	%r61, %r20, 1;
	@%p11 bra 	$L__BB8_11;

	setp.eq.s32 	%p12, %r7, 2;
	ld.global.nc.u16 	%rs6, [%rd11+2];
	st.global.u16 	[%rd12+2], %rs6;
	add.s32 	%r61, %r20, 2;
	@%p12 bra 	$L__BB8_11;

	ld.global.nc.u16 	%rs7, [%rd11+4];
	st.global.u16 	[%rd12+4], %rs7;
	add.s32 	%r61, %r20, 3;

$L__BB8_11:
	add.s32 	%r53, %r53, 1;
	setp.lt.s32 	%p13, %r53, %r28;
	@%p13 bra 	$L__BB8_4;

$L__BB8_12:
	ret;

}
	// .globl	backImageVector
.visible .entry backImageVector(
	.param .u64 backImageVector_param_0,
	.param .u64 backImageVector_param_1,
	.param .u32 backImageVector_param_2,
	.param .u32 backImageVector_param_3,
	.param .u32 backImageVector_param_4,
	.param .u32 backImageVector_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd13, [backImageVector_param_0];
	ld.param.u64 	%rd14, [backImageVector_param_1];
	ld.param.u32 	%r29, [backImageVector_param_2];
	ld.param.u32 	%r26, [backImageVector_param_3];
	ld.param.u32 	%r27, [backImageVector_param_4];
	ld.param.u32 	%r28, [backImageVector_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r33, %r31, %r30, %r32;
	mul.lo.s32 	%r1, %r33, %r28;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r37, %r35, %r34, %r36;
	mul.lo.s32 	%r2, %r37, %r28;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %ntid.z;
	mov.u32 	%r40, %tid.z;
	mad.lo.s32 	%r3, %r39, %r38, %r40;
	setp.ge.s32 	%p1, %r1, %r29;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r28;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB9_12;

	setp.lt.s32 	%p6, %r28, 1;
	@%p6 bra 	$L__BB9_12;

	mul.lo.s32 	%r41, %r28, %r27;
	mul.lo.s32 	%r42, %r41, %r28;
	add.s32 	%r43, %r1, %r3;
	mad.lo.s32 	%r4, %r43, %r26, %r2;
	setp.lt.s32 	%p7, %r27, 1;
	div.s32 	%r44, %r1, %r28;
	mul.lo.s32 	%r45, %r42, %r26;
	div.s32 	%r46, %r45, %r28;
	div.s32 	%r47, %r2, %r28;
	mul.lo.s32 	%r48, %r41, %r3;
	mad.lo.s32 	%r49, %r47, %r42, %r48;
	mad.lo.s32 	%r61, %r44, %r46, %r49;
	@%p7 bra 	$L__BB9_12;

	add.s32 	%r6, %r27, -1;
	and.b32  	%r7, %r27, 3;
	sub.s32 	%r8, %r27, %r7;
	add.s64 	%rd3, %rd2, 4;
	add.s64 	%rd4, %rd1, 4;
	mov.u32 	%r53, 0;

$L__BB9_4:
	add.s32 	%r52, %r4, %r53;
	mul.lo.s32 	%r59, %r52, %r27;
	setp.lt.u32 	%p8, %r6, 3;
	@%p8 bra 	$L__BB9_7;

	mul.wide.s32 	%rd15, %r61, 2;
	add.s64 	%rd20, %rd3, %rd15;
	mul.wide.s32 	%rd16, %r59, 2;
	add.s64 	%rd19, %rd4, %rd16;
	mov.u32 	%r57, %r8;

$L__BB9_6:
	ld.global.nc.u16 	%rs1, [%rd20+-4];
	st.global.u16 	[%rd19+-4], %rs1;
	ld.global.nc.u16 	%rs2, [%rd20+-2];
	st.global.u16 	[%rd19+-2], %rs2;
	ld.global.nc.u16 	%rs3, [%rd20];
	st.global.u16 	[%rd19], %rs3;
	ld.global.nc.u16 	%rs4, [%rd20+2];
	st.global.u16 	[%rd19+2], %rs4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r59, %r59, 4;
	add.s64 	%rd20, %rd20, 8;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r57, %r57, -4;
	setp.ne.s32 	%p9, %r57, 0;
	@%p9 bra 	$L__BB9_6;

$L__BB9_7:
	mov.u32 	%r20, %r61;
	setp.eq.s32 	%p10, %r7, 0;
	@%p10 bra 	$L__BB9_11;

	setp.eq.s32 	%p11, %r7, 1;
	mul.wide.s32 	%rd17, %r20, 2;
	add.s64 	%rd11, %rd2, %rd17;
	ld.global.nc.u16 	%rs5, [%rd11];
	mul.wide.s32 	%rd18, %r59, 2;
	add.s64 	%rd12, %rd1, %rd18;
	st.global.u16 	[%rd12], %rs5;
	add.s32 	%r61, %r20, 1;
	@%p11 bra 	$L__BB9_11;

	setp.eq.s32 	%p12, %r7, 2;
	ld.global.nc.u16 	%rs6, [%rd11+2];
	st.global.u16 	[%rd12+2], %rs6;
	add.s32 	%r61, %r20, 2;
	@%p12 bra 	$L__BB9_11;

	ld.global.nc.u16 	%rs7, [%rd11+4];
	st.global.u16 	[%rd12+4], %rs7;
	add.s32 	%r61, %r20, 3;

$L__BB9_11:
	add.s32 	%r53, %r53, 1;
	setp.lt.s32 	%p13, %r53, %r28;
	@%p13 bra 	$L__BB9_4;

$L__BB9_12:
	ret;

}
	// .globl	add3
.visible .entry add3(
	.param .u64 add3_param_0,
	.param .u64 add3_param_1,
	.param .u32 add3_param_2,
	.param .u32 add3_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [add3_param_0];
	ld.param.u64 	%rd2, [add3_param_1];
	ld.param.u32 	%r4, [add3_param_2];
	ld.param.u32 	%r3, [add3_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB10_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd4, %r11, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	add.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

$L__BB10_2:
	ret;

}
	// .globl	add3_half
.visible .entry add3_half(
	.param .u64 add3_half_param_0,
	.param .u64 add3_half_param_1,
	.param .u32 add3_half_param_2,
	.param .u32 add3_half_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [add3_half_param_0];
	ld.param.u64 	%rd2, [add3_half_param_1];
	ld.param.u32 	%r4, [add3_half_param_2];
	ld.param.u32 	%r3, [add3_half_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB11_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd4, %r11, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r2, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u16 	%rs3, [%rd8];
	// begin inline asm
	{add.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs1;

$L__BB11_2:
	ret;

}
	// .globl	dot_VectorAndMatrix
.visible .entry dot_VectorAndMatrix(
	.param .u64 dot_VectorAndMatrix_param_0,
	.param .u64 dot_VectorAndMatrix_param_1,
	.param .u64 dot_VectorAndMatrix_param_2,
	.param .u32 dot_VectorAndMatrix_param_3,
	.param .u32 dot_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd14, [dot_VectorAndMatrix_param_0];
	ld.param.u64 	%rd15, [dot_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dot_VectorAndMatrix_param_2];
	ld.param.u32 	%r16, [dot_VectorAndMatrix_param_3];
	ld.param.u32 	%r15, [dot_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r17, %ctaid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r1, %r18, %r17, %r19;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB12_9;

	mul.lo.s32 	%r27, %r1, %r15;
	setp.lt.s32 	%p2, %r15, 1;
	mov.f32 	%f29, 0f00000000;
	@%p2 bra 	$L__BB12_8;

	add.s32 	%r21, %r15, -1;
	and.b32  	%r28, %r15, 3;
	setp.lt.u32 	%p3, %r21, 3;
	mov.f32 	%f29, 0f00000000;
	mov.u32 	%r26, 0;
	@%p3 bra 	$L__BB12_5;

	sub.s32 	%r25, %r15, %r28;
	mul.wide.s32 	%rd17, %r27, 4;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd25, %rd18, 8;
	mov.u64 	%rd24, %rd2;

$L__BB12_4:
	ld.global.nc.f32 	%f12, [%rd25+-8];
	ld.global.nc.f32 	%f13, [%rd24];
	fma.rn.ftz.f32 	%f14, %f13, %f12, %f29;
	ld.global.nc.f32 	%f15, [%rd25+-4];
	ld.global.nc.f32 	%f16, [%rd24+4];
	fma.rn.ftz.f32 	%f17, %f16, %f15, %f14;
	ld.global.nc.f32 	%f18, [%rd25];
	ld.global.nc.f32 	%f19, [%rd24+8];
	fma.rn.ftz.f32 	%f20, %f19, %f18, %f17;
	ld.global.nc.f32 	%f21, [%rd25+4];
	ld.global.nc.f32 	%f22, [%rd24+12];
	fma.rn.ftz.f32 	%f29, %f22, %f21, %f20;
	add.s32 	%r26, %r26, 4;
	add.s32 	%r27, %r27, 4;
	add.s64 	%rd25, %rd25, 16;
	add.s64 	%rd24, %rd24, 16;
	add.s32 	%r25, %r25, -4;
	setp.ne.s32 	%p4, %r25, 0;
	@%p4 bra 	$L__BB12_4;

$L__BB12_5:
	setp.eq.s32 	%p5, %r28, 0;
	@%p5 bra 	$L__BB12_8;

	mul.wide.s32 	%rd19, %r26, 4;
	add.s64 	%rd27, %rd2, %rd19;
	mul.wide.s32 	%rd20, %r27, 4;
	add.s64 	%rd26, %rd1, %rd20;

$L__BB12_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f23, [%rd26];
	ld.global.nc.f32 	%f24, [%rd27];
	fma.rn.ftz.f32 	%f29, %f24, %f23, %f29;
	add.s64 	%rd27, %rd27, 4;
	add.s64 	%rd26, %rd26, 4;
	add.s32 	%r28, %r28, -1;
	setp.ne.s32 	%p6, %r28, 0;
	@%p6 bra 	$L__BB12_7;

$L__BB12_8:
	cvta.to.global.u64 	%rd21, %rd14;
	mul.wide.s32 	%rd22, %r1, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.f32 	[%rd23], %f29;

$L__BB12_9:
	ret;

}
	// .globl	dot_VectorAndMatrix_half
.visible .entry dot_VectorAndMatrix_half(
	.param .u64 dot_VectorAndMatrix_half_param_0,
	.param .u64 dot_VectorAndMatrix_half_param_1,
	.param .u64 dot_VectorAndMatrix_half_param_2,
	.param .u32 dot_VectorAndMatrix_half_param_3,
	.param .u32 dot_VectorAndMatrix_half_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd14, [dot_VectorAndMatrix_half_param_0];
	ld.param.u64 	%rd15, [dot_VectorAndMatrix_half_param_1];
	ld.param.u64 	%rd16, [dot_VectorAndMatrix_half_param_2];
	ld.param.u32 	%r12, [dot_VectorAndMatrix_half_param_3];
	ld.param.u32 	%r11, [dot_VectorAndMatrix_half_param_4];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB13_9;

	ld.const.u16 	%rs44, [sh];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB13_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r24, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r23, 0;
	@%p3 bra 	$L__BB13_5;

	sub.s32 	%r22, %r11, %r24;
	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd17, %r19, 2;
	add.s64 	%rd18, %rd1, %rd17;
	add.s64 	%rd25, %rd18, 4;
	mov.u64 	%rd24, %rd2;

$L__BB13_4:
	ld.global.nc.u16 	%rs11, [%rd24];
	ld.global.nc.u16 	%rs12, [%rd25+-4];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs44,%rs10;
}
	// end inline asm
	ld.global.nc.u16 	%rs17, [%rd24+2];
	ld.global.nc.u16 	%rs18, [%rd25+-2];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	ld.global.nc.u16 	%rs23, [%rd24+4];
	ld.global.nc.u16 	%rs24, [%rd25];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd24+6];
	ld.global.nc.u16 	%rs30, [%rd25+2];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd25, %rd25, 8;
	add.s64 	%rd24, %rd24, 8;
	add.s32 	%r22, %r22, -4;
	setp.ne.s32 	%p4, %r22, 0;
	@%p4 bra 	$L__BB13_4;

$L__BB13_5:
	setp.eq.s32 	%p5, %r24, 0;
	@%p5 bra 	$L__BB13_8;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd19, %r20, 2;
	add.s64 	%rd27, %rd1, %rd19;
	mul.wide.s32 	%rd20, %r23, 2;
	add.s64 	%rd26, %rd2, %rd20;

$L__BB13_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs35, [%rd26];
	ld.global.nc.u16 	%rs36, [%rd27];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs44,%rs34;
}
	// end inline asm
	add.s64 	%rd27, %rd27, 2;
	add.s64 	%rd26, %rd26, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB13_7;

$L__BB13_8:
	cvta.to.global.u64 	%rd21, %rd14;
	mul.wide.s32 	%rd22, %r1, 2;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u16 	[%rd23], %rs44;

$L__BB13_9:
	ret;

}
	// .globl	MatMulKernel
.visible .entry MatMulKernel(
	.param .u64 MatMulKernel_param_0,
	.param .u64 MatMulKernel_param_1,
	.param .u64 MatMulKernel_param_2,
	.param .u32 MatMulKernel_param_3,
	.param .u32 MatMulKernel_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<48>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<27>;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE8blockElt;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE9blockxInd;
	// demoted variable
	.shared .align 4 .u32 _ZZ12MatMulKernelE9blockyInd;
	// demoted variable
	.shared .align 2 .b8 _ZZ12MatMulKernelE1b[128];

	ld.param.u64 	%rd11, [MatMulKernel_param_0];
	ld.param.u64 	%rd12, [MatMulKernel_param_1];
	ld.param.u64 	%rd13, [MatMulKernel_param_2];
	ld.param.u32 	%r26, [MatMulKernel_param_3];
	ld.param.u32 	%r27, [MatMulKernel_param_4];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB14_4;

	mov.u32 	%r29, %ctaid.x;
	shl.b32 	%r2, %r29, 6;
	add.s32 	%r30, %r2, 64;
	mov.u32 	%r54, 64;
	setp.le.u32 	%p2, %r30, %r27;
	@%p2 bra 	$L__BB14_3;

	shr.s32 	%r31, %r27, 31;
	shr.u32 	%r32, %r31, 26;
	add.s32 	%r33, %r27, %r32;
	and.b32  	%r34, %r33, -64;
	sub.s32 	%r54, %r27, %r34;

$L__BB14_3:
	st.shared.u32 	[_ZZ12MatMulKernelE8blockElt], %r54;
	st.shared.u32 	[_ZZ12MatMulKernelE9blockxInd], %r2;
	mov.u32 	%r35, %ctaid.y;
	shl.b32 	%r36, %r35, 10;
	st.shared.u32 	[_ZZ12MatMulKernelE9blockyInd], %r36;

$L__BB14_4:
	bar.sync 	0;
	ld.shared.u32 	%r37, [_ZZ12MatMulKernelE8blockElt];
	setp.ge.u32 	%p3, %r1, %r37;
	@%p3 bra 	$L__BB14_6;

	ld.shared.u32 	%r38, [_ZZ12MatMulKernelE9blockxInd];
	add.s32 	%r39, %r38, %r1;
	cvta.to.global.u64 	%rd14, %rd12;
	mul.wide.u32 	%rd15, %r39, 2;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u16 	%rs9, [%rd16];
	shl.b32 	%r40, %r1, 1;
	mov.u32 	%r41, _ZZ12MatMulKernelE1b;
	add.s32 	%r42, %r41, %r40;
	st.shared.u16 	[%r42], %rs9;

$L__BB14_6:
	bar.sync 	0;
	ld.const.u16 	%rs47, [sh];
	ld.shared.u32 	%r43, [_ZZ12MatMulKernelE9blockyInd];
	add.s32 	%r5, %r43, %r1;
	setp.ge.s32 	%p4, %r5, %r26;
	@%p4 bra 	$L__BB14_15;

	ld.shared.u32 	%r6, [_ZZ12MatMulKernelE8blockElt];
	setp.lt.s32 	%p5, %r6, 1;
	@%p5 bra 	$L__BB14_14;

	ld.shared.u32 	%r7, [_ZZ12MatMulKernelE9blockxInd];
	mov.u32 	%r59, 0;
	and.b32  	%r61, %r6, 3;
	add.s32 	%r45, %r6, -1;
	setp.lt.u32 	%p6, %r45, 3;
	@%p6 bra 	$L__BB14_11;

	sub.s32 	%r58, %r6, %r61;
	add.s32 	%r48, %r7, 1;
	mad.lo.s32 	%r55, %r26, %r48, %r5;
	mad.lo.s32 	%r49, %r7, %r26, %r5;
	mul.wide.s32 	%rd17, %r49, 2;
	add.s64 	%rd25, %rd1, %rd17;
	shl.b32 	%r11, %r26, 2;
	mul.wide.s32 	%rd3, %r11, 2;
	mul.wide.s32 	%rd4, %r26, 2;
	mov.u32 	%r56, _ZZ12MatMulKernelE1b;

$L__BB14_10:
	ld.shared.u16 	%rs12, [%r56];
	ld.global.u16 	%rs13, [%rd25];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs47,%rs11;
}
	// end inline asm
	ld.shared.u16 	%rs18, [%r56+2];
	mul.wide.s32 	%rd18, %r55, 2;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.u16 	%rs19, [%rd19];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.shared.u16 	%rs24, [%r56+4];
	add.s64 	%rd20, %rd19, %rd4;
	ld.global.u16 	%rs25, [%rd20];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.shared.u16 	%rs30, [%r56+6];
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.u16 	%rs31, [%rd21];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r59, %r59, 4;
	add.s32 	%r56, %r56, 8;
	add.s32 	%r55, %r55, %r11;
	add.s64 	%rd25, %rd25, %rd3;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p7, %r58, 0;
	@%p7 bra 	$L__BB14_10;

$L__BB14_11:
	setp.eq.s32 	%p8, %r61, 0;
	@%p8 bra 	$L__BB14_14;

	shl.b32 	%r50, %r59, 1;
	mov.u32 	%r51, _ZZ12MatMulKernelE1b;
	add.s32 	%r60, %r51, %r50;
	add.s32 	%r52, %r59, %r7;
	mad.lo.s32 	%r53, %r26, %r52, %r5;
	mul.wide.s32 	%rd22, %r53, 2;
	add.s64 	%rd26, %rd1, %rd22;
	mul.wide.s32 	%rd8, %r26, 2;

$L__BB14_13:
	.pragma "nounroll";
	ld.shared.u16 	%rs36, [%r60];
	ld.global.u16 	%rs37, [%rd26];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs47,%rs35;
}
	// end inline asm
	add.s32 	%r60, %r60, 2;
	add.s64 	%rd26, %rd26, %rd8;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p9, %r61, 0;
	@%p9 bra 	$L__BB14_13;

$L__BB14_14:
	mul.wide.s32 	%rd24, %r5, 2;
	add.s64 	%rd23, %rd11, %rd24;
	// begin inline asm
	{ atom.add.noftz.f16 %rs41,[%rd23],%rs47; }

	// end inline asm

$L__BB14_15:
	ret;

}
	// .globl	MatMulKernelT
.visible .entry MatMulKernelT(
	.param .u64 MatMulKernelT_param_0,
	.param .u64 MatMulKernelT_param_1,
	.param .u64 MatMulKernelT_param_2,
	.param .u32 MatMulKernelT_param_3,
	.param .u32 MatMulKernelT_param_4
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<48>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<21>;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE8blockElt;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE9blockxInd;
	// demoted variable
	.shared .align 4 .u32 _ZZ13MatMulKernelTE9blockyInd;
	// demoted variable
	.shared .align 2 .b8 _ZZ13MatMulKernelTE1b[128];

	ld.param.u64 	%rd8, [MatMulKernelT_param_0];
	ld.param.u64 	%rd9, [MatMulKernelT_param_1];
	ld.param.u64 	%rd10, [MatMulKernelT_param_2];
	ld.param.u32 	%r22, [MatMulKernelT_param_3];
	ld.param.u32 	%r23, [MatMulKernelT_param_4];
	cvta.to.global.u64 	%rd1, %rd10;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB15_4;

	mov.u32 	%r25, %ctaid.y;
	shl.b32 	%r2, %r25, 6;
	add.s32 	%r26, %r2, 64;
	mov.u32 	%r49, 64;
	setp.le.u32 	%p2, %r26, %r22;
	@%p2 bra 	$L__BB15_3;

	shr.s32 	%r27, %r22, 31;
	shr.u32 	%r28, %r27, 26;
	add.s32 	%r29, %r22, %r28;
	and.b32  	%r30, %r29, -64;
	sub.s32 	%r49, %r22, %r30;

$L__BB15_3:
	st.shared.u32 	[_ZZ13MatMulKernelTE8blockElt], %r49;
	mov.u32 	%r31, %ctaid.x;
	shl.b32 	%r32, %r31, 10;
	st.shared.u32 	[_ZZ13MatMulKernelTE9blockxInd], %r32;
	st.shared.u32 	[_ZZ13MatMulKernelTE9blockyInd], %r2;

$L__BB15_4:
	bar.sync 	0;
	ld.shared.u32 	%r33, [_ZZ13MatMulKernelTE8blockElt];
	setp.ge.u32 	%p3, %r1, %r33;
	@%p3 bra 	$L__BB15_6;

	ld.shared.u32 	%r34, [_ZZ13MatMulKernelTE9blockyInd];
	add.s32 	%r35, %r34, %r1;
	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r35, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u16 	%rs9, [%rd13];
	shl.b32 	%r36, %r1, 1;
	mov.u32 	%r37, _ZZ13MatMulKernelTE1b;
	add.s32 	%r38, %r37, %r36;
	st.shared.u16 	[%r38], %rs9;

$L__BB15_6:
	bar.sync 	0;
	ld.const.u16 	%rs47, [sh];
	ld.shared.u32 	%r39, [_ZZ13MatMulKernelTE9blockxInd];
	add.s32 	%r5, %r39, %r1;
	setp.ge.s32 	%p4, %r5, %r23;
	@%p4 bra 	$L__BB15_15;

	ld.shared.u32 	%r6, [_ZZ13MatMulKernelTE8blockElt];
	setp.lt.s32 	%p5, %r6, 1;
	@%p5 bra 	$L__BB15_14;

	ld.shared.u32 	%r7, [_ZZ13MatMulKernelTE9blockyInd];
	mov.u32 	%r53, 0;
	and.b32  	%r55, %r6, 3;
	add.s32 	%r41, %r6, -1;
	setp.lt.u32 	%p6, %r41, 3;
	@%p6 bra 	$L__BB15_11;

	sub.s32 	%r52, %r6, %r55;
	mad.lo.s32 	%r44, %r22, %r5, %r7;
	mul.wide.s32 	%rd14, %r44, 2;
	add.s64 	%rd15, %rd1, %rd14;
	add.s64 	%rd19, %rd15, 4;
	mov.u32 	%r50, _ZZ13MatMulKernelTE1b;

$L__BB15_10:
	ld.shared.u16 	%rs12, [%r50];
	ld.global.u16 	%rs13, [%rd19+-4];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs13;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs47,%rs11;
}
	// end inline asm
	ld.shared.u16 	%rs18, [%r50+2];
	ld.global.u16 	%rs19, [%rd19+-2];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs14,%rs17;
}
	// end inline asm
	ld.shared.u16 	%rs24, [%r50+4];
	ld.global.u16 	%rs25, [%rd19];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs25;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs20,%rs23;
}
	// end inline asm
	ld.shared.u16 	%rs30, [%r50+6];
	ld.global.u16 	%rs31, [%rd19+2];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs26,%rs29;
}
	// end inline asm
	add.s32 	%r53, %r53, 4;
	add.s64 	%rd19, %rd19, 8;
	add.s32 	%r50, %r50, 8;
	add.s32 	%r52, %r52, -4;
	setp.ne.s32 	%p7, %r52, 0;
	@%p7 bra 	$L__BB15_10;

$L__BB15_11:
	setp.eq.s32 	%p8, %r55, 0;
	@%p8 bra 	$L__BB15_14;

	shl.b32 	%r45, %r53, 1;
	mov.u32 	%r46, _ZZ13MatMulKernelTE1b;
	add.s32 	%r54, %r46, %r45;
	add.s32 	%r47, %r53, %r7;
	mad.lo.s32 	%r48, %r22, %r5, %r47;
	mul.wide.s32 	%rd16, %r48, 2;
	add.s64 	%rd20, %rd1, %rd16;

$L__BB15_13:
	.pragma "nounroll";
	ld.shared.u16 	%rs36, [%r54];
	ld.global.u16 	%rs37, [%rd20];
	// begin inline asm
	{mul.f16 %rs35,%rs36,%rs37;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs47,%rs47,%rs35;
}
	// end inline asm
	add.s32 	%r54, %r54, 2;
	add.s64 	%rd20, %rd20, 2;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p9, %r55, 0;
	@%p9 bra 	$L__BB15_13;

$L__BB15_14:
	mul.wide.s32 	%rd18, %r5, 2;
	add.s64 	%rd17, %rd8, %rd18;
	// begin inline asm
	{ atom.add.noftz.f16 %rs41,[%rd17],%rs47; }

	// end inline asm

$L__BB15_15:
	ret;

}
	// .globl	dotT_VectorAndMatrix
.visible .entry dotT_VectorAndMatrix(
	.param .u64 dotT_VectorAndMatrix_param_0,
	.param .u64 dotT_VectorAndMatrix_param_1,
	.param .u64 dotT_VectorAndMatrix_param_2,
	.param .u32 dotT_VectorAndMatrix_param_3,
	.param .u32 dotT_VectorAndMatrix_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd17, [dotT_VectorAndMatrix_param_0];
	ld.param.u64 	%rd18, [dotT_VectorAndMatrix_param_1];
	ld.param.u64 	%rd16, [dotT_VectorAndMatrix_param_2];
	ld.param.u32 	%r11, [dotT_VectorAndMatrix_param_3];
	ld.param.u32 	%r12, [dotT_VectorAndMatrix_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB16_9;

	ld.const.u16 	%rs1, [sh];
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs1;}

	// end inline asm
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB16_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB16_5;

	sub.s32 	%r21, %r11, %r23;
	mul.wide.s32 	%rd19, %r1, 4;
	add.s64 	%rd29, %rd1, %rd19;
	mul.wide.s32 	%rd4, %r12, 4;
	mov.u64 	%rd28, %rd2;

$L__BB16_4:
	ld.global.nc.f32 	%f11, [%rd29];
	ld.global.nc.f32 	%f12, [%rd28];
	fma.rn.ftz.f32 	%f13, %f12, %f11, %f28;
	add.s64 	%rd20, %rd29, %rd4;
	ld.global.nc.f32 	%f14, [%rd20];
	ld.global.nc.f32 	%f15, [%rd28+4];
	fma.rn.ftz.f32 	%f16, %f15, %f14, %f13;
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.nc.f32 	%f17, [%rd21];
	ld.global.nc.f32 	%f18, [%rd28+8];
	fma.rn.ftz.f32 	%f19, %f18, %f17, %f16;
	add.s64 	%rd22, %rd21, %rd4;
	add.s64 	%rd29, %rd22, %rd4;
	ld.global.nc.f32 	%f20, [%rd22];
	ld.global.nc.f32 	%f21, [%rd28+12];
	fma.rn.ftz.f32 	%f28, %f21, %f20, %f19;
	add.s32 	%r22, %r22, 4;
	add.s64 	%rd28, %rd28, 16;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB16_4;

$L__BB16_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB16_8;

	mul.wide.s32 	%rd23, %r22, 4;
	add.s64 	%rd31, %rd2, %rd23;
	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd24, %r19, 4;
	add.s64 	%rd30, %rd1, %rd24;
	mul.wide.s32 	%rd11, %r12, 4;

$L__BB16_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f22, [%rd30];
	ld.global.nc.f32 	%f23, [%rd31];
	fma.rn.ftz.f32 	%f28, %f23, %f22, %f28;
	add.s64 	%rd31, %rd31, 4;
	add.s64 	%rd30, %rd30, %rd11;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB16_7;

$L__BB16_8:
	cvta.to.global.u64 	%rd25, %rd16;
	mul.wide.s32 	%rd26, %r1, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f28;

$L__BB16_9:
	ret;

}
	// .globl	dotT_VectorAndMatrix_half
.visible .entry dotT_VectorAndMatrix_half(
	.param .u64 dotT_VectorAndMatrix_half_param_0,
	.param .u64 dotT_VectorAndMatrix_half_param_1,
	.param .u64 dotT_VectorAndMatrix_half_param_2,
	.param .u32 dotT_VectorAndMatrix_half_param_3,
	.param .u32 dotT_VectorAndMatrix_half_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd17, [dotT_VectorAndMatrix_half_param_0];
	ld.param.u64 	%rd18, [dotT_VectorAndMatrix_half_param_1];
	ld.param.u64 	%rd16, [dotT_VectorAndMatrix_half_param_2];
	ld.param.u32 	%r11, [dotT_VectorAndMatrix_half_param_3];
	ld.param.u32 	%r12, [dotT_VectorAndMatrix_half_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB17_9;

	ld.const.u16 	%rs44, [sh];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB17_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB17_5;

	sub.s32 	%r21, %r11, %r23;
	mul.wide.s32 	%rd19, %r1, 2;
	add.s64 	%rd29, %rd1, %rd19;
	mul.wide.s32 	%rd4, %r12, 2;
	mov.u64 	%rd28, %rd2;

$L__BB17_4:
	ld.global.nc.u16 	%rs11, [%rd28];
	ld.global.nc.u16 	%rs12, [%rd29];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs44,%rs10;
}
	// end inline asm
	ld.global.nc.u16 	%rs17, [%rd28+2];
	add.s64 	%rd20, %rd29, %rd4;
	ld.global.nc.u16 	%rs18, [%rd20];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	ld.global.nc.u16 	%rs23, [%rd28+4];
	add.s64 	%rd21, %rd20, %rd4;
	ld.global.nc.u16 	%rs24, [%rd21];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	ld.global.nc.u16 	%rs29, [%rd28+6];
	add.s64 	%rd22, %rd21, %rd4;
	add.s64 	%rd29, %rd22, %rd4;
	ld.global.nc.u16 	%rs30, [%rd22];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r22, %r22, 4;
	add.s64 	%rd28, %rd28, 8;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB17_4;

$L__BB17_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB17_8;

	mul.wide.s32 	%rd23, %r22, 2;
	add.s64 	%rd31, %rd2, %rd23;
	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd24, %r19, 2;
	add.s64 	%rd30, %rd1, %rd24;
	mul.wide.s32 	%rd11, %r12, 2;

$L__BB17_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs35, [%rd31];
	ld.global.nc.u16 	%rs36, [%rd30];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs44,%rs44,%rs34;
}
	// end inline asm
	add.s64 	%rd31, %rd31, 2;
	add.s64 	%rd30, %rd30, %rd11;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB17_7;

$L__BB17_8:
	cvta.to.global.u64 	%rd25, %rd16;
	mul.wide.s32 	%rd26, %r1, 2;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u16 	[%rd27], %rs44;

$L__BB17_9:
	ret;

}
	// .globl	derivativeWeight
.visible .entry derivativeWeight(
	.param .u64 derivativeWeight_param_0,
	.param .u64 derivativeWeight_param_1,
	.param .u64 derivativeWeight_param_2,
	.param .u32 derivativeWeight_param_3,
	.param .u32 derivativeWeight_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [derivativeWeight_param_0];
	ld.param.u64 	%rd2, [derivativeWeight_param_1];
	ld.param.u64 	%rd3, [derivativeWeight_param_2];
	ld.param.u32 	%r4, [derivativeWeight_param_3];
	ld.param.u32 	%r3, [derivativeWeight_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB18_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	mul.wide.s32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd1;
	mul.wide.s32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.f32 	%f1, [%rd12];
	ld.global.nc.f32 	%f2, [%rd9];
	ld.global.f32 	%f3, [%rd6];
	fma.rn.ftz.f32 	%f4, %f2, %f1, %f3;
	st.global.f32 	[%rd6], %f4;

$L__BB18_2:
	ret;

}
	// .globl	derivativeWeight_half
.visible .entry derivativeWeight_half(
	.param .u64 derivativeWeight_half_param_0,
	.param .u64 derivativeWeight_half_param_1,
	.param .u64 derivativeWeight_half_param_2,
	.param .u32 derivativeWeight_half_param_3,
	.param .u32 derivativeWeight_half_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [derivativeWeight_half_param_0];
	ld.param.u64 	%rd2, [derivativeWeight_half_param_1];
	ld.param.u64 	%rd3, [derivativeWeight_half_param_2];
	ld.param.u32 	%r4, [derivativeWeight_half_param_3];
	ld.param.u32 	%r3, [derivativeWeight_half_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ctaid.y;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r4;
	setp.ge.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB19_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mad.lo.s32 	%r11, %r1, %r3, %r2;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs2, [%rd7];
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r2, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs3, [%rd10];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	mul.wide.s32 	%rd11, %r11, 2;
	add.s64 	%rd12, %rd4, %rd11;
	ld.global.u16 	%rs5, [%rd12];
	// begin inline asm
	{add.f16 %rs4,%rs5,%rs1;
}
	// end inline asm
	st.global.u16 	[%rd12], %rs4;

$L__BB19_2:
	ret;

}
	// .globl	addMatrix
.visible .entry addMatrix(
	.param .u64 addMatrix_param_0,
	.param .u64 addMatrix_param_1,
	.param .u32 addMatrix_param_2,
	.param .u32 addMatrix_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd13, [addMatrix_param_0];
	ld.param.u64 	%rd12, [addMatrix_param_1];
	ld.param.u32 	%r11, [addMatrix_param_2];
	ld.param.u32 	%r12, [addMatrix_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB20_9;

	cvta.to.global.u64 	%rd14, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd15, %r1, 4;
	add.s64 	%rd3, %rd14, %rd15;
	ld.global.f32 	%f22, [%rd3];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB20_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB20_5;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd16, %rd2, 2;
	add.s64 	%rd21, %rd1, %rd16;
	mul.wide.s32 	%rd5, %r12, 4;

$L__BB20_4:
	ld.global.nc.f32 	%f10, [%rd21];
	add.ftz.f32 	%f11, %f22, %f10;
	add.s64 	%rd17, %rd21, %rd5;
	ld.global.nc.f32 	%f12, [%rd17];
	add.ftz.f32 	%f13, %f11, %f12;
	add.s64 	%rd18, %rd17, %rd5;
	ld.global.nc.f32 	%f14, [%rd18];
	add.ftz.f32 	%f15, %f13, %f14;
	add.s64 	%rd19, %rd18, %rd5;
	add.s64 	%rd21, %rd19, %rd5;
	ld.global.nc.f32 	%f16, [%rd19];
	add.ftz.f32 	%f22, %f15, %f16;
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB20_4;

$L__BB20_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB20_8;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd20, %r19, 4;
	add.s64 	%rd22, %rd1, %rd20;
	mul.wide.s32 	%rd9, %r12, 4;

$L__BB20_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f17, [%rd22];
	add.ftz.f32 	%f22, %f22, %f17;
	add.s64 	%rd22, %rd22, %rd9;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB20_7;

$L__BB20_8:
	st.global.f32 	[%rd3], %f22;

$L__BB20_9:
	ret;

}
	// .globl	addMatrix_half
.visible .entry addMatrix_half(
	.param .u64 addMatrix_half_param_0,
	.param .u64 addMatrix_half_param_1,
	.param .u32 addMatrix_half_param_2,
	.param .u32 addMatrix_half_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<30>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd13, [addMatrix_half_param_0];
	ld.param.u64 	%rd12, [addMatrix_half_param_1];
	ld.param.u32 	%r11, [addMatrix_half_param_2];
	ld.param.u32 	%r12, [addMatrix_half_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB21_9;

	cvta.to.global.u64 	%rd14, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd15, %r1, 2;
	add.s64 	%rd3, %rd14, %rd15;
	ld.global.u16 	%rs29, [%rd3];
	setp.lt.s32 	%p2, %r11, 1;
	@%p2 bra 	$L__BB21_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r23, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.u32 	%r22, 0;
	@%p3 bra 	$L__BB21_5;

	sub.s32 	%r21, %r11, %r23;
	shl.b64 	%rd16, %rd2, 1;
	add.s64 	%rd21, %rd1, %rd16;
	mul.wide.s32 	%rd5, %r12, 2;

$L__BB21_4:
	ld.global.nc.u16 	%rs12, [%rd21];
	// begin inline asm
	{add.f16 %rs10,%rs29,%rs12;
}
	// end inline asm
	add.s64 	%rd17, %rd21, %rd5;
	ld.global.nc.u16 	%rs15, [%rd17];
	// begin inline asm
	{add.f16 %rs13,%rs10,%rs15;
}
	// end inline asm
	add.s64 	%rd18, %rd17, %rd5;
	ld.global.nc.u16 	%rs18, [%rd18];
	// begin inline asm
	{add.f16 %rs16,%rs13,%rs18;
}
	// end inline asm
	add.s64 	%rd19, %rd18, %rd5;
	add.s64 	%rd21, %rd19, %rd5;
	ld.global.nc.u16 	%rs21, [%rd19];
	// begin inline asm
	{add.f16 %rs29,%rs16,%rs21;
}
	// end inline asm
	add.s32 	%r22, %r22, 4;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p4, %r21, 0;
	@%p4 bra 	$L__BB21_4;

$L__BB21_5:
	setp.eq.s32 	%p5, %r23, 0;
	@%p5 bra 	$L__BB21_8;

	mad.lo.s32 	%r19, %r22, %r12, %r1;
	mul.wide.s32 	%rd20, %r19, 2;
	add.s64 	%rd22, %rd1, %rd20;
	mul.wide.s32 	%rd9, %r12, 2;

$L__BB21_7:
	.pragma "nounroll";
	ld.global.nc.u16 	%rs24, [%rd22];
	// begin inline asm
	{add.f16 %rs29,%rs29,%rs24;
}
	// end inline asm
	add.s64 	%rd22, %rd22, %rd9;
	add.s32 	%r23, %r23, -1;
	setp.ne.s32 	%p6, %r23, 0;
	@%p6 bra 	$L__BB21_7;

$L__BB21_8:
	st.global.u16 	[%rd3], %rs29;

$L__BB21_9:
	ret;

}
	// .globl	reduceMaxIdxOptimizedShared
.visible .entry reduceMaxIdxOptimizedShared(
	.param .u64 reduceMaxIdxOptimizedShared_param_0,
	.param .u32 reduceMaxIdxOptimizedShared_param_1,
	.param .u64 reduceMaxIdxOptimizedShared_param_2,
	.param .u64 reduceMaxIdxOptimizedShared_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .f32 _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax;
	// demoted variable
	.shared .align 4 .u32 _ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx;

	ld.param.u64 	%rd2, [reduceMaxIdxOptimizedShared_param_0];
	ld.param.u32 	%r12, [reduceMaxIdxOptimizedShared_param_1];
	ld.param.u64 	%rd3, [reduceMaxIdxOptimizedShared_param_2];
	ld.param.u64 	%rd4, [reduceMaxIdxOptimizedShared_param_3];
	mov.u32 	%r18, %tid.x;
	setp.ne.s32 	%p1, %r18, 0;
	@%p1 bra 	$L__BB22_2;

	mov.u32 	%r13, 0;
	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax], %r13;
	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx], %r13;

$L__BB22_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r18, %r12;
	mov.u32 	%r20, 0;
	mov.f32 	%f12, 0f00000000;
	@%p2 bra 	$L__BB22_5;

	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB22_4:
	mul.wide.s32 	%rd5, %r18, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f7, [%rd6];
	setp.lt.ftz.f32 	%p3, %f12, %f7;
	selp.f32 	%f12, %f7, %f12, %p3;
	selp.b32 	%r20, %r18, %r20, %p3;
	add.s32 	%r18, %r18, %r2;
	setp.lt.s32 	%p4, %r18, %r12;
	@%p4 bra 	$L__BB22_4;

$L__BB22_5:
	ld.shared.f32 	%f4, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	setp.ge.ftz.f32 	%p5, %f4, %f12;
	@%p5 bra 	$L__BB22_9;

	mov.b32 	%r21, %f4;
	mov.b32 	%r9, %f12;

$L__BB22_7:
	mov.b32 	%f8, %r21;
	setp.le.ftz.f32 	%p6, %f12, %f8;
	@%p6 bra 	$L__BB22_9;

	mov.u32 	%r16, _ZZ27reduceMaxIdxOptimizedSharedE9sharedMax;
	atom.shared.cas.b32 	%r11, [%r16], %r21, %r9;
	setp.ne.s32 	%p7, %r21, %r11;
	mov.u32 	%r21, %r11;
	@%p7 bra 	$L__BB22_7;

$L__BB22_9:
	bar.sync 	0;
	ld.shared.f32 	%f9, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	setp.neu.ftz.f32 	%p8, %f9, %f12;
	@%p8 bra 	$L__BB22_11;

	st.shared.u32 	[_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx], %r20;

$L__BB22_11:
	bar.sync 	0;
	@%p1 bra 	$L__BB22_13;

	ld.shared.f32 	%f10, [_ZZ27reduceMaxIdxOptimizedSharedE9sharedMax];
	cvta.to.global.u64 	%rd7, %rd3;
	st.global.f32 	[%rd7], %f10;
	ld.shared.u32 	%r17, [_ZZ27reduceMaxIdxOptimizedSharedE12sharedMaxIdx];
	cvta.to.global.u64 	%rd8, %rd4;
	st.global.u32 	[%rd8], %r17;

$L__BB22_13:
	ret;

}
	// .globl	reverse
.visible .entry reverse(
	.param .u64 reverse_param_0,
	.param .u32 reverse_param_1,
	.param .u32 reverse_param_2,
	.param .u32 reverse_param_3
)
{



	ret;

}
	// .globl	sharedMem_transpose
.visible .entry sharedMem_transpose(
	.param .u64 sharedMem_transpose_param_0,
	.param .u64 sharedMem_transpose_param_1,
	.param .u32 sharedMem_transpose_param_2,
	.param .u32 sharedMem_transpose_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ19sharedMem_transposeE8M_Shared[4096];

	ld.param.u64 	%rd1, [sharedMem_transpose_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 7;
	mov.u32 	%r14, _ZZ19sharedMem_transposeE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 2;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB24_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	st.shared.f32 	[%r3], %f1;

$L__BB24_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB24_4;

	ld.shared.f32 	%f2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

$L__BB24_4:
	ret;

}
	// .globl	sharedMem_transpose_half
.visible .entry sharedMem_transpose_half(
	.param .u64 sharedMem_transpose_half_param_0,
	.param .u64 sharedMem_transpose_half_param_1,
	.param .u32 sharedMem_transpose_half_param_2,
	.param .u32 sharedMem_transpose_half_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ24sharedMem_transpose_halfE8M_Shared[2048];

	ld.param.u64 	%rd1, [sharedMem_transpose_half_param_0];
	ld.param.u64 	%rd2, [sharedMem_transpose_half_param_1];
	ld.param.u32 	%r5, [sharedMem_transpose_half_param_2];
	ld.param.u32 	%r6, [sharedMem_transpose_half_param_3];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	shl.b32 	%r11, %r10, 5;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r2, %r11, %r12;
	setp.lt.s32 	%p2, %r1, %r5;
	setp.lt.s32 	%p3, %r2, %r6;
	and.pred  	%p1, %p2, %p3;
	shl.b32 	%r13, %r12, 6;
	mov.u32 	%r14, _ZZ24sharedMem_transpose_halfE8M_Shared;
	add.s32 	%r15, %r14, %r13;
	shl.b32 	%r16, %r9, 1;
	add.s32 	%r3, %r15, %r16;
	not.pred 	%p4, %p1;
	@%p4 bra 	$L__BB25_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r17, %r1, %r6, %r2;
	mul.wide.s32 	%rd4, %r17, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	st.shared.u16 	[%r3], %rs1;

$L__BB25_2:
	bar.sync 	0;
	mad.lo.s32 	%r4, %r2, %r5, %r1;
	@%p4 bra 	$L__BB25_4;

	ld.shared.u16 	%rs2, [%r3];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r4, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB25_4:
	ret;

}
	// .globl	matrixTransposeSolveBankConflicts
.visible .entry matrixTransposeSolveBankConflicts(
	.param .u64 matrixTransposeSolveBankConflicts_param_0,
	.param .u64 matrixTransposeSolveBankConflicts_param_1,
	.param .u32 matrixTransposeSolveBankConflicts_param_2,
	.param .u32 matrixTransposeSolveBankConflicts_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ33matrixTransposeSolveBankConflictsE3mat[2112];

	ld.param.u64 	%rd1, [matrixTransposeSolveBankConflicts_param_0];
	ld.param.u64 	%rd2, [matrixTransposeSolveBankConflicts_param_1];
	ld.param.u32 	%r7, [matrixTransposeSolveBankConflicts_param_2];
	ld.param.u32 	%r8, [matrixTransposeSolveBankConflicts_param_3];
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 5;
	mov.u32 	%r11, %ctaid.y;
	shl.b32 	%r12, %r11, 5;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r2, %r12, %r1;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r10, %r3;
	add.s32 	%r5, %r10, %r1;
	add.s32 	%r6, %r12, %r3;
	setp.ge.s32 	%p1, %r2, %r7;
	setp.ge.s32 	%p2, %r4, %r8;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB26_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r13, %r2, %r8, %r4;
	mul.wide.s32 	%rd4, %r13, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	mov.u32 	%r14, _ZZ33matrixTransposeSolveBankConflictsE3mat;
	mad.lo.s32 	%r15, %r1, 66, %r14;
	shl.b32 	%r16, %r3, 1;
	add.s32 	%r17, %r15, %r16;
	st.shared.u16 	[%r17], %rs1;

$L__BB26_2:
	bar.sync 	0;
	setp.ge.s32 	%p4, %r5, %r7;
	setp.ge.s32 	%p5, %r6, %r8;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB26_4;

	mad.lo.s32 	%r18, %r5, %r7, %r6;
	mov.u32 	%r19, _ZZ33matrixTransposeSolveBankConflictsE3mat;
	mad.lo.s32 	%r20, %r3, 66, %r19;
	shl.b32 	%r21, %r1, 1;
	add.s32 	%r22, %r20, %r21;
	ld.shared.u16 	%rs2, [%r22];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB26_4:
	ret;

}
	// .globl	transposeV3
.visible .entry transposeV3(
	.param .u64 transposeV3_param_0,
	.param .u64 transposeV3_param_1,
	.param .u32 transposeV3_param_2,
	.param .u32 transposeV3_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 2 .b8 _ZZ11transposeV3E3s_A[2112];

	ld.param.u64 	%rd1, [transposeV3_param_0];
	ld.param.u64 	%rd2, [transposeV3_param_1];
	ld.param.u32 	%r9, [transposeV3_param_2];
	ld.param.u32 	%r10, [transposeV3_param_3];
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %ntid.y;
	mul.lo.s32 	%r4, %r14, %r13;
	mov.u32 	%r5, %tid.y;
	add.s32 	%r6, %r4, %r5;
	setp.ge.s32 	%p1, %r3, %r9;
	setp.ge.s32 	%p2, %r6, %r10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB27_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mad.lo.s32 	%r15, %r6, %r10, %r3;
	mul.wide.s32 	%rd4, %r15, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	mov.u32 	%r16, _ZZ11transposeV3E3s_A;
	mad.lo.s32 	%r17, %r5, 66, %r16;
	shl.b32 	%r18, %r2, 1;
	add.s32 	%r19, %r17, %r18;
	st.shared.u16 	[%r19], %rs1;

$L__BB27_2:
	bar.sync 	0;
	add.s32 	%r7, %r4, %r2;
	setp.ge.s32 	%p4, %r7, %r9;
	add.s32 	%r8, %r1, %r5;
	setp.ge.s32 	%p5, %r8, %r10;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB27_4;

	mad.lo.s32 	%r20, %r8, %r9, %r7;
	mov.u32 	%r21, _ZZ11transposeV3E3s_A;
	mad.lo.s32 	%r22, %r2, 66, %r21;
	shl.b32 	%r23, %r5, 1;
	add.s32 	%r24, %r22, %r23;
	ld.shared.u16 	%rs2, [%r24];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r20, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

$L__BB27_4:
	ret;

}
	// .globl	matrixDiv
.visible .entry matrixDiv(
	.param .u64 matrixDiv_param_0,
	.param .align 2 .b8 matrixDiv_param_1[2],
	.param .u32 matrixDiv_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [matrixDiv_param_1];
	ld.param.u64 	%rd2, [matrixDiv_param_0];
	ld.param.u32 	%r2, [matrixDiv_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB28_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs6, [%rd1];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs6;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs5;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs9,%rs19;
}
	// end inline asm
	mov.u16 	%rs13, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs9, %rs13;
  selp.u16 %rs11, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs11, 0;
	@%p2 bra 	$L__BB28_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs14, %rs9;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs15, 0;
	@%p3 bra 	$L__BB28_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f11;}

	// end inline asm

$L__BB28_4:
	st.global.u16 	[%rd1], %rs19;

$L__BB28_5:
	ret;

}
	// .globl	matrixDiv_float
.visible .entry matrixDiv_float(
	.param .u64 matrixDiv_float_param_0,
	.param .f32 matrixDiv_float_param_1,
	.param .u32 matrixDiv_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [matrixDiv_float_param_0];
	ld.param.f32 	%f1, [matrixDiv_float_param_1];
	ld.param.u32 	%r2, [matrixDiv_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB29_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	div.approx.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB29_2:
	ret;

}
	// .globl	addCopy
.visible .entry addCopy(
	.param .u64 addCopy_param_0,
	.param .u64 addCopy_param_1,
	.param .u32 addCopy_param_2,
	.param .u32 addCopy_param_3,
	.param .u32 addCopy_param_4,
	.param .u32 addCopy_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_param_0];
	ld.param.u64 	%rd2, [addCopy_param_1];
	ld.param.u32 	%r6, [addCopy_param_2];
	ld.param.u32 	%r3, [addCopy_param_3];
	ld.param.u32 	%r4, [addCopy_param_4];
	ld.param.u32 	%r5, [addCopy_param_5];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB30_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r15, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB30_2:
	ret;

}
	// .globl	addCopy_half
.visible .entry addCopy_half(
	.param .u64 addCopy_half_param_0,
	.param .u64 addCopy_half_param_1,
	.param .u32 addCopy_half_param_2,
	.param .u32 addCopy_half_param_3,
	.param .u32 addCopy_half_param_4,
	.param .u32 addCopy_half_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addCopy_half_param_0];
	ld.param.u64 	%rd2, [addCopy_half_param_1];
	ld.param.u32 	%r6, [addCopy_half_param_2];
	ld.param.u32 	%r3, [addCopy_half_param_3];
	ld.param.u32 	%r4, [addCopy_half_param_4];
	ld.param.u32 	%r5, [addCopy_half_param_5];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB31_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r15, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r14, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB31_2:
	ret;

}
	// .globl	NormalizationLayerForward2D
.visible .entry NormalizationLayerForward2D(
	.param .u64 NormalizationLayerForward2D_param_0,
	.param .u64 NormalizationLayerForward2D_param_1,
	.param .u64 NormalizationLayerForward2D_param_2,
	.param .u32 NormalizationLayerForward2D_param_3,
	.param .u32 NormalizationLayerForward2D_param_4,
	.param .u32 NormalizationLayerForward2D_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<99>;


	ld.param.u64 	%rd45, [NormalizationLayerForward2D_param_0];
	ld.param.u64 	%rd46, [NormalizationLayerForward2D_param_1];
	ld.param.u64 	%rd47, [NormalizationLayerForward2D_param_2];
	ld.param.u32 	%r35, [NormalizationLayerForward2D_param_3];
	ld.param.u32 	%r34, [NormalizationLayerForward2D_param_4];
	ld.param.u32 	%r36, [NormalizationLayerForward2D_param_5];
	cvta.to.global.u64 	%rd1, %rd47;
	cvta.to.global.u64 	%rd2, %rd46;
	cvta.to.global.u64 	%rd3, %rd45;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r38, %r37, %r39;
	mov.u32 	%r40, %ctaid.y;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r42, %tid.y;
	mad.lo.s32 	%r2, %r41, %r40, %r42;
	setp.ge.s32 	%p1, %r1, %r36;
	setp.ge.s32 	%p2, %r2, %r35;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB32_22;

	ld.global.u64 	%rd48, [%rd3];
	cvta.to.global.u64 	%rd49, %rd48;
	cvt.s64.s32 	%rd4, %r1;
	mul.wide.s32 	%rd50, %r1, 8;
	add.s64 	%rd51, %rd49, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	cvta.to.global.u64 	%rd5, %rd52;
	ld.global.u64 	%rd53, [%rd3+8];
	cvta.to.global.u64 	%rd54, %rd53;
	add.s64 	%rd55, %rd54, %rd50;
	ld.global.u64 	%rd56, [%rd55];
	cvta.to.global.u64 	%rd57, %rd56;
	cvt.s64.s32 	%rd6, %r2;
	mul.wide.s32 	%rd58, %r2, 4;
	add.s64 	%rd7, %rd57, %rd58;
	ld.global.f32 	%f80, [%rd7];
	mul.lo.s32 	%r60, %r2, %r34;
	setp.lt.s32 	%p4, %r34, 1;
	@%p4 bra 	$L__BB32_8;

	add.s32 	%r43, %r34, -1;
	and.b32  	%r51, %r34, 3;
	setp.lt.u32 	%p5, %r43, 3;
	mov.u32 	%r50, %r60;
	@%p5 bra 	$L__BB32_5;

	sub.s32 	%r49, %r34, %r51;
	mul.wide.s32 	%rd59, %r60, 4;
	add.s64 	%rd60, %rd5, %rd59;
	add.s64 	%rd87, %rd60, 8;
	mov.u32 	%r50, %r60;

$L__BB32_4:
	ld.global.f32 	%f21, [%rd87+-8];
	add.ftz.f32 	%f22, %f80, %f21;
	ld.global.f32 	%f23, [%rd87+-4];
	add.ftz.f32 	%f24, %f22, %f23;
	ld.global.f32 	%f25, [%rd87];
	add.ftz.f32 	%f26, %f24, %f25;
	ld.global.f32 	%f27, [%rd87+4];
	add.ftz.f32 	%f80, %f26, %f27;
	add.s32 	%r50, %r50, 4;
	add.s64 	%rd87, %rd87, 16;
	add.s32 	%r49, %r49, -4;
	setp.ne.s32 	%p6, %r49, 0;
	@%p6 bra 	$L__BB32_4;

$L__BB32_5:
	setp.eq.s32 	%p7, %r51, 0;
	@%p7 bra 	$L__BB32_8;

	mul.wide.s32 	%rd61, %r50, 4;
	add.s64 	%rd88, %rd5, %rd61;

$L__BB32_7:
	.pragma "nounroll";
	ld.global.f32 	%f28, [%rd88];
	add.ftz.f32 	%f80, %f80, %f28;
	add.s64 	%rd88, %rd88, 4;
	add.s32 	%r51, %r51, -1;
	setp.ne.s32 	%p8, %r51, 0;
	@%p8 bra 	$L__BB32_7;

$L__BB32_8:
	cvt.rn.f32.s32 	%f9, %r34;
	div.approx.ftz.f32 	%f29, %f80, %f9;
	st.global.f32 	[%rd7], %f29;
	ld.global.u64 	%rd62, [%rd3+16];
	cvta.to.global.u64 	%rd63, %rd62;
	shl.b64 	%rd64, %rd4, 3;
	add.s64 	%rd65, %rd63, %rd64;
	ld.global.u64 	%rd66, [%rd65];
	cvta.to.global.u64 	%rd67, %rd66;
	shl.b64 	%rd68, %rd6, 2;
	add.s64 	%rd15, %rd67, %rd68;
	ld.global.f32 	%f85, [%rd15];
	ld.global.u64 	%rd69, [%rd3+8];
	cvta.to.global.u64 	%rd70, %rd69;
	add.s64 	%rd71, %rd70, %rd64;
	ld.global.u64 	%rd72, [%rd71];
	cvta.to.global.u64 	%rd73, %rd72;
	add.s64 	%rd74, %rd73, %rd68;
	ld.global.f32 	%f11, [%rd74];
	@%p4 bra 	$L__BB32_15;

	add.s32 	%r44, %r34, -1;
	and.b32  	%r55, %r34, 3;
	setp.lt.u32 	%p10, %r44, 3;
	mov.u32 	%r54, %r60;
	@%p10 bra 	$L__BB32_12;

	sub.s32 	%r53, %r34, %r55;
	mul.wide.s32 	%rd75, %r60, 4;
	add.s64 	%rd76, %rd5, %rd75;
	add.s64 	%rd89, %rd76, 8;
	mov.u32 	%r54, %r60;

$L__BB32_11:
	ld.global.f32 	%f31, [%rd89+-8];
	sub.ftz.f32 	%f32, %f31, %f11;
	fma.rn.ftz.f32 	%f33, %f32, %f32, %f85;
	ld.global.f32 	%f34, [%rd89+-4];
	sub.ftz.f32 	%f35, %f34, %f11;
	fma.rn.ftz.f32 	%f36, %f35, %f35, %f33;
	ld.global.f32 	%f37, [%rd89];
	sub.ftz.f32 	%f38, %f37, %f11;
	fma.rn.ftz.f32 	%f39, %f38, %f38, %f36;
	ld.global.f32 	%f40, [%rd89+4];
	sub.ftz.f32 	%f41, %f40, %f11;
	fma.rn.ftz.f32 	%f85, %f41, %f41, %f39;
	add.s32 	%r54, %r54, 4;
	add.s64 	%rd89, %rd89, 16;
	add.s32 	%r53, %r53, -4;
	setp.ne.s32 	%p11, %r53, 0;
	@%p11 bra 	$L__BB32_11;

$L__BB32_12:
	setp.eq.s32 	%p12, %r55, 0;
	@%p12 bra 	$L__BB32_15;

	mul.wide.s32 	%rd77, %r54, 4;
	add.s64 	%rd90, %rd5, %rd77;

$L__BB32_14:
	.pragma "nounroll";
	ld.global.f32 	%f42, [%rd90];
	sub.ftz.f32 	%f43, %f42, %f11;
	fma.rn.ftz.f32 	%f85, %f43, %f43, %f85;
	add.s64 	%rd90, %rd90, 4;
	add.s32 	%r55, %r55, -1;
	setp.ne.s32 	%p13, %r55, 0;
	@%p13 bra 	$L__BB32_14;

$L__BB32_15:
	div.approx.ftz.f32 	%f44, %f85, %f9;
	st.global.f32 	[%rd15], %f44;
	add.ftz.f32 	%f45, %f44, 0f33D6BF95;
	sqrt.approx.ftz.f32 	%f19, %f45;
	ld.global.u64 	%rd78, [%rd3+24];
	cvta.to.global.u64 	%rd79, %rd78;
	add.s64 	%rd81, %rd79, %rd64;
	ld.global.u64 	%rd82, [%rd81];
	cvta.to.global.u64 	%rd22, %rd82;
	@%p4 bra 	$L__BB32_22;

	add.s32 	%r46, %r34, -1;
	and.b32  	%r61, %r34, 3;
	setp.lt.u32 	%p15, %r46, 3;
	mov.u32 	%r59, 0;
	@%p15 bra 	$L__BB32_19;

	sub.s32 	%r58, %r34, %r61;
	mul.wide.s32 	%rd83, %r60, 4;
	add.s64 	%rd84, %rd83, 8;
	add.s64 	%rd94, %rd5, %rd84;
	add.s64 	%rd93, %rd22, %rd84;
	mov.u64 	%rd91, %rd2;
	mov.u64 	%rd92, %rd1;

$L__BB32_18:
	ld.global.f32 	%f46, [%rd94+-8];
	sub.ftz.f32 	%f47, %f46, %f11;
	div.approx.ftz.f32 	%f48, %f47, %f19;
	ld.global.nc.f32 	%f49, [%rd91];
	ld.global.nc.f32 	%f50, [%rd92];
	fma.rn.ftz.f32 	%f51, %f48, %f49, %f50;
	st.global.f32 	[%rd93+-8], %f51;
	ld.global.f32 	%f52, [%rd94+-4];
	sub.ftz.f32 	%f53, %f52, %f11;
	div.approx.ftz.f32 	%f54, %f53, %f19;
	ld.global.nc.f32 	%f55, [%rd91+4];
	ld.global.nc.f32 	%f56, [%rd92+4];
	fma.rn.ftz.f32 	%f57, %f54, %f55, %f56;
	st.global.f32 	[%rd93+-4], %f57;
	ld.global.f32 	%f58, [%rd94];
	sub.ftz.f32 	%f59, %f58, %f11;
	div.approx.ftz.f32 	%f60, %f59, %f19;
	ld.global.nc.f32 	%f61, [%rd91+8];
	ld.global.nc.f32 	%f62, [%rd92+8];
	fma.rn.ftz.f32 	%f63, %f60, %f61, %f62;
	st.global.f32 	[%rd93], %f63;
	ld.global.f32 	%f64, [%rd94+4];
	sub.ftz.f32 	%f65, %f64, %f11;
	div.approx.ftz.f32 	%f66, %f65, %f19;
	ld.global.nc.f32 	%f67, [%rd91+12];
	ld.global.nc.f32 	%f68, [%rd92+12];
	fma.rn.ftz.f32 	%f69, %f66, %f67, %f68;
	st.global.f32 	[%rd93+4], %f69;
	add.s32 	%r59, %r59, 4;
	add.s32 	%r60, %r60, 4;
	add.s64 	%rd94, %rd94, 16;
	add.s64 	%rd93, %rd93, 16;
	add.s64 	%rd92, %rd92, 16;
	add.s64 	%rd91, %rd91, 16;
	add.s32 	%r58, %r58, -4;
	setp.ne.s32 	%p16, %r58, 0;
	@%p16 bra 	$L__BB32_18;

$L__BB32_19:
	setp.eq.s32 	%p17, %r61, 0;
	@%p17 bra 	$L__BB32_22;

	mul.wide.s32 	%rd85, %r59, 4;
	add.s64 	%rd98, %rd1, %rd85;
	add.s64 	%rd97, %rd2, %rd85;
	mul.wide.s32 	%rd86, %r60, 4;
	add.s64 	%rd96, %rd22, %rd86;
	add.s64 	%rd95, %rd5, %rd86;

$L__BB32_21:
	.pragma "nounroll";
	ld.global.f32 	%f70, [%rd95];
	sub.ftz.f32 	%f71, %f70, %f11;
	div.approx.ftz.f32 	%f72, %f71, %f19;
	ld.global.nc.f32 	%f73, [%rd97];
	ld.global.nc.f32 	%f74, [%rd98];
	fma.rn.ftz.f32 	%f75, %f72, %f73, %f74;
	st.global.f32 	[%rd96], %f75;
	add.s64 	%rd98, %rd98, 4;
	add.s64 	%rd97, %rd97, 4;
	add.s64 	%rd96, %rd96, 4;
	add.s64 	%rd95, %rd95, 4;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p18, %r61, 0;
	@%p18 bra 	$L__BB32_21;

$L__BB32_22:
	ret;

}
	// .globl	NormalizationLayerBackward2D
.visible .entry NormalizationLayerBackward2D(
	.param .u64 NormalizationLayerBackward2D_param_0,
	.param .u64 NormalizationLayerBackward2D_param_1,
	.param .u64 NormalizationLayerBackward2D_param_2,
	.param .u64 NormalizationLayerBackward2D_param_3,
	.param .u64 NormalizationLayerBackward2D_param_4,
	.param .u32 NormalizationLayerBackward2D_param_5,
	.param .u32 NormalizationLayerBackward2D_param_6,
	.param .u32 NormalizationLayerBackward2D_param_7,
	.param .u32 NormalizationLayerBackward2D_param_8,
	.param .u32 NormalizationLayerBackward2D_param_9
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<189>;


	ld.param.u64 	%rd98, [NormalizationLayerBackward2D_param_0];
	ld.param.u64 	%rd100, [NormalizationLayerBackward2D_param_1];
	ld.param.u64 	%rd99, [NormalizationLayerBackward2D_param_2];
	ld.param.u64 	%rd101, [NormalizationLayerBackward2D_param_3];
	ld.param.u64 	%rd102, [NormalizationLayerBackward2D_param_4];
	ld.param.u32 	%r53, [NormalizationLayerBackward2D_param_7];
	ld.param.u32 	%r52, [NormalizationLayerBackward2D_param_8];
	ld.param.u32 	%r54, [NormalizationLayerBackward2D_param_9];
	cvta.to.global.u64 	%rd1, %rd101;
	cvta.to.global.u64 	%rd2, %rd100;
	cvta.to.global.u64 	%rd3, %rd102;
	mov.u32 	%r55, %ctaid.x;
	mov.u32 	%r56, %ntid.x;
	mov.u32 	%r57, %tid.x;
	mad.lo.s32 	%r1, %r56, %r55, %r57;
	mov.u32 	%r58, %ctaid.y;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %tid.y;
	mad.lo.s32 	%r2, %r59, %r58, %r60;
	setp.ge.s32 	%p1, %r1, %r54;
	setp.ge.s32 	%p2, %r2, %r53;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB33_28;

	cvta.to.global.u64 	%rd103, %rd98;
	ld.global.u64 	%rd104, [%rd103];
	cvta.to.global.u64 	%rd105, %rd104;
	mul.wide.s32 	%rd106, %r1, 8;
	add.s64 	%rd107, %rd105, %rd106;
	ld.global.u64 	%rd108, [%rd107];
	cvta.to.global.u64 	%rd4, %rd108;
	ld.global.u64 	%rd109, [%rd103+8];
	cvta.to.global.u64 	%rd110, %rd109;
	add.s64 	%rd111, %rd110, %rd106;
	ld.global.u64 	%rd112, [%rd111];
	cvta.to.global.u64 	%rd113, %rd112;
	mul.wide.s32 	%rd114, %r2, 4;
	add.s64 	%rd115, %rd113, %rd114;
	ld.global.u64 	%rd116, [%rd103+16];
	cvta.to.global.u64 	%rd117, %rd116;
	add.s64 	%rd118, %rd117, %rd106;
	ld.global.u64 	%rd119, [%rd118];
	cvta.to.global.u64 	%rd5, %rd119;
	ld.global.u64 	%rd120, [%rd103+24];
	cvta.to.global.u64 	%rd121, %rd120;
	add.s64 	%rd122, %rd121, %rd106;
	ld.global.u64 	%rd123, [%rd122];
	cvta.to.global.u64 	%rd124, %rd123;
	add.s64 	%rd125, %rd124, %rd114;
	ld.global.f32 	%f1, [%rd125];
	ld.global.u64 	%rd126, [%rd103+32];
	cvta.to.global.u64 	%rd127, %rd126;
	add.s64 	%rd128, %rd127, %rd106;
	ld.global.u64 	%rd129, [%rd128];
	cvta.to.global.u64 	%rd6, %rd129;
	ld.global.u64 	%rd130, [%rd103+40];
	cvta.to.global.u64 	%rd131, %rd130;
	add.s64 	%rd132, %rd131, %rd106;
	ld.global.u64 	%rd133, [%rd132];
	cvta.to.global.u64 	%rd7, %rd133;
	ld.global.f32 	%f2, [%rd115];
	mul.lo.s32 	%r93, %r2, %r52;
	setp.lt.s32 	%p4, %r52, 1;
	mov.f32 	%f211, 0f00000000;
	mov.f32 	%f202, %f211;
	@%p4 bra 	$L__BB33_8;

	add.s32 	%r62, %r52, -1;
	and.b32  	%r76, %r52, 3;
	setp.lt.u32 	%p5, %r62, 3;
	mov.f32 	%f202, 0f00000000;
	mov.u32 	%r74, 0;
	mov.u32 	%r75, %r93;
	@%p5 bra 	$L__BB33_5;

	sub.s32 	%r73, %r52, %r76;
	mul.wide.s32 	%rd134, %r93, 4;
	add.s64 	%rd135, %rd134, 8;
	add.s64 	%rd159, %rd4, %rd135;
	add.s64 	%rd158, %rd5, %rd135;
	mov.u64 	%rd157, %rd2;
	mov.u32 	%r75, %r93;

$L__BB33_4:
	ld.global.nc.f32 	%f31, [%rd157];
	ld.global.f32 	%f32, [%rd159+-8];
	mul.ftz.f32 	%f33, %f32, %f31;
	ld.global.f32 	%f34, [%rd158+-8];
	sub.ftz.f32 	%f35, %f34, %f1;
	fma.rn.ftz.f32 	%f36, %f33, %f35, %f202;
	ld.global.nc.f32 	%f37, [%rd157+4];
	ld.global.f32 	%f38, [%rd159+-4];
	mul.ftz.f32 	%f39, %f38, %f37;
	ld.global.f32 	%f40, [%rd158+-4];
	sub.ftz.f32 	%f41, %f40, %f1;
	fma.rn.ftz.f32 	%f42, %f39, %f41, %f36;
	ld.global.nc.f32 	%f43, [%rd157+8];
	ld.global.f32 	%f44, [%rd159];
	mul.ftz.f32 	%f45, %f44, %f43;
	ld.global.f32 	%f46, [%rd158];
	sub.ftz.f32 	%f47, %f46, %f1;
	fma.rn.ftz.f32 	%f48, %f45, %f47, %f42;
	ld.global.nc.f32 	%f49, [%rd157+12];
	ld.global.f32 	%f50, [%rd159+4];
	mul.ftz.f32 	%f51, %f50, %f49;
	ld.global.f32 	%f52, [%rd158+4];
	sub.ftz.f32 	%f53, %f52, %f1;
	fma.rn.ftz.f32 	%f202, %f51, %f53, %f48;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r75, %r75, 4;
	add.s64 	%rd159, %rd159, 16;
	add.s64 	%rd158, %rd158, 16;
	add.s64 	%rd157, %rd157, 16;
	add.s32 	%r73, %r73, -4;
	setp.ne.s32 	%p6, %r73, 0;
	@%p6 bra 	$L__BB33_4;

$L__BB33_5:
	setp.eq.s32 	%p7, %r76, 0;
	@%p7 bra 	$L__BB33_8;

	mul.wide.s32 	%rd136, %r74, 4;
	add.s64 	%rd162, %rd2, %rd136;
	mul.wide.s32 	%rd137, %r75, 4;
	add.s64 	%rd161, %rd5, %rd137;
	add.s64 	%rd160, %rd4, %rd137;

$L__BB33_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f54, [%rd162];
	ld.global.f32 	%f55, [%rd160];
	mul.ftz.f32 	%f56, %f55, %f54;
	ld.global.f32 	%f57, [%rd161];
	sub.ftz.f32 	%f58, %f57, %f1;
	fma.rn.ftz.f32 	%f202, %f56, %f58, %f202;
	add.s64 	%rd162, %rd162, 4;
	add.s64 	%rd161, %rd161, 4;
	add.s64 	%rd160, %rd160, 4;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p8, %r76, 0;
	@%p8 bra 	$L__BB33_7;

$L__BB33_8:
	mov.f32 	%f212, %f211;
	@%p4 bra 	$L__BB33_15;

	add.s32 	%r65, %r52, -1;
	and.b32  	%r82, %r52, 3;
	setp.lt.u32 	%p10, %r65, 3;
	mov.f32 	%f212, 0f00000000;
	mov.u32 	%r80, 0;
	mov.u32 	%r81, %r93;
	mov.f32 	%f211, %f212;
	@%p10 bra 	$L__BB33_12;

	sub.s32 	%r79, %r52, %r82;
	mul.wide.s32 	%rd138, %r93, 4;
	add.s64 	%rd139, %rd138, 8;
	add.s64 	%rd165, %rd4, %rd139;
	add.s64 	%rd164, %rd5, %rd139;
	mov.u64 	%rd163, %rd2;
	mov.u32 	%r81, %r93;

$L__BB33_11:
	ld.global.nc.f32 	%f66, [%rd163];
	ld.global.f32 	%f67, [%rd165+-8];
	fma.rn.ftz.f32 	%f68, %f67, %f66, %f212;
	ld.global.f32 	%f69, [%rd164+-8];
	sub.ftz.f32 	%f70, %f69, %f1;
	add.ftz.f32 	%f71, %f211, %f70;
	ld.global.nc.f32 	%f72, [%rd163+4];
	ld.global.f32 	%f73, [%rd165+-4];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f68;
	ld.global.f32 	%f75, [%rd164+-4];
	sub.ftz.f32 	%f76, %f75, %f1;
	add.ftz.f32 	%f77, %f71, %f76;
	ld.global.nc.f32 	%f78, [%rd163+8];
	ld.global.f32 	%f79, [%rd165];
	fma.rn.ftz.f32 	%f80, %f79, %f78, %f74;
	ld.global.f32 	%f81, [%rd164];
	sub.ftz.f32 	%f82, %f81, %f1;
	add.ftz.f32 	%f83, %f77, %f82;
	ld.global.nc.f32 	%f84, [%rd163+12];
	ld.global.f32 	%f85, [%rd165+4];
	fma.rn.ftz.f32 	%f212, %f85, %f84, %f80;
	ld.global.f32 	%f86, [%rd164+4];
	sub.ftz.f32 	%f87, %f86, %f1;
	add.ftz.f32 	%f211, %f83, %f87;
	add.s32 	%r80, %r80, 4;
	add.s32 	%r81, %r81, 4;
	add.s64 	%rd165, %rd165, 16;
	add.s64 	%rd164, %rd164, 16;
	add.s64 	%rd163, %rd163, 16;
	add.s32 	%r79, %r79, -4;
	setp.ne.s32 	%p11, %r79, 0;
	@%p11 bra 	$L__BB33_11;

$L__BB33_12:
	setp.eq.s32 	%p12, %r82, 0;
	@%p12 bra 	$L__BB33_15;

	mul.wide.s32 	%rd140, %r80, 4;
	add.s64 	%rd168, %rd2, %rd140;
	mul.wide.s32 	%rd141, %r81, 4;
	add.s64 	%rd167, %rd5, %rd141;
	add.s64 	%rd166, %rd4, %rd141;

$L__BB33_14:
	.pragma "nounroll";
	ld.global.nc.f32 	%f88, [%rd168];
	ld.global.f32 	%f89, [%rd166];
	fma.rn.ftz.f32 	%f212, %f89, %f88, %f212;
	ld.global.f32 	%f90, [%rd167];
	sub.ftz.f32 	%f91, %f90, %f1;
	add.ftz.f32 	%f211, %f211, %f91;
	add.s64 	%rd168, %rd168, 4;
	add.s64 	%rd167, %rd167, 4;
	add.s64 	%rd166, %rd166, 4;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p13, %r82, 0;
	@%p13 bra 	$L__BB33_14;

$L__BB33_15:
	add.ftz.f32 	%f92, %f2, 0f33D6BF95;
	lg2.approx.ftz.f32 	%f93, %f92;
	mul.ftz.f32 	%f94, %f93, 0fBFC00000;
	ex2.approx.ftz.f32 	%f95, %f94;
	mul.ftz.f32 	%f96, %f95, 0fBF000000;
	mul.ftz.f32 	%f97, %f96, %f202;
	sqrt.approx.ftz.f32 	%f98, %f92;
	mul.ftz.f32 	%f99, %f97, 0fC0000000;
	mul.ftz.f32 	%f100, %f99, %f211;
	cvt.rn.f32.s32 	%f101, %r52;
	div.approx.ftz.f32 	%f102, %f100, %f101;
	mov.f32 	%f103, 0fBF800000;
	div.approx.ftz.f32 	%f104, %f103, %f98;
	fma.rn.ftz.f32 	%f105, %f104, %f212, %f102;
	div.approx.ftz.f32 	%f24, %f105, %f101;
	mov.f32 	%f106, 0f40000000;
	div.approx.ftz.f32 	%f107, %f106, %f101;
	mul.ftz.f32 	%f25, %f97, %f107;
	rsqrt.approx.ftz.f32 	%f26, %f92;
	@%p4 bra 	$L__BB33_28;

	add.s32 	%r28, %r52, -1;
	and.b32  	%r94, %r52, 3;
	setp.lt.u32 	%p15, %r28, 3;
	mov.u32 	%r86, 0;
	mov.u32 	%r87, %r93;
	@%p15 bra 	$L__BB33_19;

	sub.s32 	%r85, %r52, %r94;
	mul.wide.s32 	%rd142, %r93, 4;
	add.s64 	%rd143, %rd142, 8;
	add.s64 	%rd172, %rd4, %rd143;
	add.s64 	%rd171, %rd5, %rd143;
	add.s64 	%rd169, %rd6, %rd143;
	mov.u64 	%rd170, %rd2;
	mov.u32 	%r87, %r93;

$L__BB33_18:
	ld.global.nc.f32 	%f108, [%rd170];
	ld.global.f32 	%f109, [%rd172+-8];
	mul.ftz.f32 	%f110, %f109, %f108;
	ld.global.f32 	%f111, [%rd171+-8];
	sub.ftz.f32 	%f112, %f111, %f1;
	mul.ftz.f32 	%f113, %f25, %f112;
	fma.rn.ftz.f32 	%f114, %f26, %f110, %f113;
	add.ftz.f32 	%f115, %f24, %f114;
	st.global.f32 	[%rd169+-8], %f115;
	ld.global.nc.f32 	%f116, [%rd170+4];
	ld.global.f32 	%f117, [%rd172+-4];
	mul.ftz.f32 	%f118, %f117, %f116;
	ld.global.f32 	%f119, [%rd171+-4];
	sub.ftz.f32 	%f120, %f119, %f1;
	mul.ftz.f32 	%f121, %f25, %f120;
	fma.rn.ftz.f32 	%f122, %f26, %f118, %f121;
	add.ftz.f32 	%f123, %f24, %f122;
	st.global.f32 	[%rd169+-4], %f123;
	ld.global.nc.f32 	%f124, [%rd170+8];
	ld.global.f32 	%f125, [%rd172];
	mul.ftz.f32 	%f126, %f125, %f124;
	ld.global.f32 	%f127, [%rd171];
	sub.ftz.f32 	%f128, %f127, %f1;
	mul.ftz.f32 	%f129, %f25, %f128;
	fma.rn.ftz.f32 	%f130, %f26, %f126, %f129;
	add.ftz.f32 	%f131, %f24, %f130;
	st.global.f32 	[%rd169], %f131;
	ld.global.nc.f32 	%f132, [%rd170+12];
	ld.global.f32 	%f133, [%rd172+4];
	mul.ftz.f32 	%f134, %f133, %f132;
	ld.global.f32 	%f135, [%rd171+4];
	sub.ftz.f32 	%f136, %f135, %f1;
	mul.ftz.f32 	%f137, %f25, %f136;
	fma.rn.ftz.f32 	%f138, %f26, %f134, %f137;
	add.ftz.f32 	%f139, %f24, %f138;
	st.global.f32 	[%rd169+4], %f139;
	add.s32 	%r86, %r86, 4;
	add.s32 	%r87, %r87, 4;
	add.s64 	%rd172, %rd172, 16;
	add.s64 	%rd171, %rd171, 16;
	add.s64 	%rd170, %rd170, 16;
	add.s64 	%rd169, %rd169, 16;
	add.s32 	%r85, %r85, -4;
	setp.ne.s32 	%p16, %r85, 0;
	@%p16 bra 	$L__BB33_18;

$L__BB33_19:
	setp.eq.s32 	%p17, %r94, 0;
	@%p17 bra 	$L__BB33_22;

	mul.wide.s32 	%rd144, %r86, 4;
	add.s64 	%rd176, %rd2, %rd144;
	mul.wide.s32 	%rd145, %r87, 4;
	add.s64 	%rd175, %rd6, %rd145;
	add.s64 	%rd174, %rd5, %rd145;
	add.s64 	%rd173, %rd4, %rd145;
	mov.u32 	%r88, %r94;

$L__BB33_21:
	.pragma "nounroll";
	ld.global.nc.f32 	%f140, [%rd176];
	ld.global.f32 	%f141, [%rd173];
	mul.ftz.f32 	%f142, %f141, %f140;
	ld.global.f32 	%f143, [%rd174];
	sub.ftz.f32 	%f144, %f143, %f1;
	mul.ftz.f32 	%f145, %f25, %f144;
	fma.rn.ftz.f32 	%f146, %f26, %f142, %f145;
	add.ftz.f32 	%f147, %f24, %f146;
	st.global.f32 	[%rd175], %f147;
	add.s64 	%rd176, %rd176, 4;
	add.s64 	%rd175, %rd175, 4;
	add.s64 	%rd174, %rd174, 4;
	add.s64 	%rd173, %rd173, 4;
	add.s32 	%r88, %r88, -1;
	setp.ne.s32 	%p18, %r88, 0;
	@%p18 bra 	$L__BB33_21;

$L__BB33_22:
	mov.u32 	%r92, 0;
	@%p15 bra 	$L__BB33_25;

	sub.s32 	%r91, %r52, %r94;
	mul.wide.s32 	%rd146, %r93, 4;
	add.s64 	%rd147, %rd146, 8;
	add.s64 	%rd182, %rd4, %rd147;
	add.s64 	%rd181, %rd7, %rd147;
	cvta.to.global.u64 	%rd178, %rd99;
	mov.u64 	%rd177, %rd1;
	mov.u64 	%rd179, %rd3;
	mov.u64 	%rd180, %rd2;

$L__BB33_24:
	ld.global.f32 	%f148, [%rd182+-8];
	atom.global.add.f32 	%f149, [%rd179], %f148;
	ld.global.nc.f32 	%f150, [%rd178];
	ld.global.f32 	%f151, [%rd181+-8];
	sub.ftz.f32 	%f152, %f151, %f150;
	ld.global.nc.f32 	%f153, [%rd180];
	div.approx.ftz.f32 	%f154, %f152, %f153;
	ld.global.f32 	%f155, [%rd182+-8];
	mul.ftz.f32 	%f156, %f155, %f154;
	atom.global.add.f32 	%f157, [%rd177], %f156;
	ld.global.f32 	%f158, [%rd182+-4];
	add.s64 	%rd148, %rd179, 4;
	atom.global.add.f32 	%f159, [%rd148], %f158;
	ld.global.nc.f32 	%f160, [%rd178+4];
	ld.global.f32 	%f161, [%rd181+-4];
	sub.ftz.f32 	%f162, %f161, %f160;
	ld.global.nc.f32 	%f163, [%rd180+4];
	div.approx.ftz.f32 	%f164, %f162, %f163;
	ld.global.f32 	%f165, [%rd182+-4];
	mul.ftz.f32 	%f166, %f165, %f164;
	add.s64 	%rd149, %rd177, 4;
	atom.global.add.f32 	%f167, [%rd149], %f166;
	ld.global.f32 	%f168, [%rd182];
	add.s64 	%rd150, %rd179, 8;
	atom.global.add.f32 	%f169, [%rd150], %f168;
	ld.global.nc.f32 	%f170, [%rd178+8];
	ld.global.f32 	%f171, [%rd181];
	sub.ftz.f32 	%f172, %f171, %f170;
	ld.global.nc.f32 	%f173, [%rd180+8];
	div.approx.ftz.f32 	%f174, %f172, %f173;
	ld.global.f32 	%f175, [%rd182];
	mul.ftz.f32 	%f176, %f175, %f174;
	add.s64 	%rd151, %rd177, 8;
	atom.global.add.f32 	%f177, [%rd151], %f176;
	ld.global.f32 	%f178, [%rd182+4];
	add.s64 	%rd152, %rd179, 12;
	atom.global.add.f32 	%f179, [%rd152], %f178;
	ld.global.nc.f32 	%f180, [%rd178+12];
	ld.global.f32 	%f181, [%rd181+4];
	sub.ftz.f32 	%f182, %f181, %f180;
	ld.global.nc.f32 	%f183, [%rd180+12];
	div.approx.ftz.f32 	%f184, %f182, %f183;
	ld.global.f32 	%f185, [%rd182+4];
	mul.ftz.f32 	%f186, %f185, %f184;
	add.s64 	%rd153, %rd177, 12;
	atom.global.add.f32 	%f187, [%rd153], %f186;
	add.s32 	%r92, %r92, 4;
	add.s32 	%r93, %r93, 4;
	add.s64 	%rd182, %rd182, 16;
	add.s64 	%rd181, %rd181, 16;
	add.s64 	%rd180, %rd180, 16;
	add.s64 	%rd179, %rd179, 16;
	add.s64 	%rd178, %rd178, 16;
	add.s64 	%rd177, %rd177, 16;
	add.s32 	%r91, %r91, -4;
	setp.ne.s32 	%p20, %r91, 0;
	@%p20 bra 	$L__BB33_24;

$L__BB33_25:
	@%p17 bra 	$L__BB33_28;

	mul.wide.s32 	%rd154, %r92, 4;
	add.s64 	%rd188, %rd2, %rd154;
	cvta.to.global.u64 	%rd155, %rd99;
	add.s64 	%rd187, %rd155, %rd154;
	add.s64 	%rd186, %rd1, %rd154;
	add.s64 	%rd185, %rd3, %rd154;
	mul.wide.s32 	%rd156, %r93, 4;
	add.s64 	%rd184, %rd7, %rd156;
	add.s64 	%rd183, %rd4, %rd156;

$L__BB33_27:
	.pragma "nounroll";
	ld.global.f32 	%f188, [%rd183];
	atom.global.add.f32 	%f189, [%rd185], %f188;
	ld.global.nc.f32 	%f190, [%rd187];
	ld.global.f32 	%f191, [%rd184];
	sub.ftz.f32 	%f192, %f191, %f190;
	ld.global.nc.f32 	%f193, [%rd188];
	div.approx.ftz.f32 	%f194, %f192, %f193;
	ld.global.f32 	%f195, [%rd183];
	mul.ftz.f32 	%f196, %f195, %f194;
	atom.global.add.f32 	%f197, [%rd186], %f196;
	add.s64 	%rd188, %rd188, 4;
	add.s64 	%rd187, %rd187, 4;
	add.s64 	%rd186, %rd186, 4;
	add.s64 	%rd185, %rd185, 4;
	add.s64 	%rd184, %rd184, 4;
	add.s64 	%rd183, %rd183, 4;
	add.s32 	%r94, %r94, -1;
	setp.ne.s32 	%p22, %r94, 0;
	@%p22 bra 	$L__BB33_27;

$L__BB33_28:
	ret;

}
	// .globl	dropout
.visible .entry dropout(
	.param .u64 dropout_param_0,
	.param .u64 dropout_param_1,
	.param .align 2 .b8 dropout_param_2[2],
	.param .u32 dropout_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<30>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs6, [dropout_param_2];
	ld.param.u64 	%rd2, [dropout_param_0];
	ld.param.u64 	%rd3, [dropout_param_1];
	ld.param.u32 	%r2, [dropout_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB34_6;

	ld.const.u16 	%rs8, [sh+8];
	// begin inline asm
	{sub.f16 %rs7,%rs8,%rs6;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs8;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs7;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f7, %f6;
}
	// end inline asm
	mul.ftz.f32 	%f9, %f5, %f7;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f9;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs13,%rs29;
}
	// end inline asm
	mov.u16 	%rs17, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs13, %rs17;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs15, 0;
	@%p2 bra 	$L__BB34_4;

	mov.f32 	%f10, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f10;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs13;
  selp.u16 %rs19, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs19, 0;
	@%p3 bra 	$L__BB34_4;

	neg.ftz.f32 	%f12, %f6;
	fma.rn.ftz.f32 	%f13, %f12, %f9, %f5;
	fma.rn.ftz.f32 	%f11, %f7, %f13, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f11;}

	// end inline asm

$L__BB34_4:
	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs24, [%rd6];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs24, %rs6;
  selp.u16 %rs23, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs23, 0;
	@%p4 bra 	$L__BB34_6;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u16 	%rs27, [%rd9];
	// begin inline asm
	{mul.f16 %rs26,%rs27,%rs29;
}
	// end inline asm
	st.global.u16 	[%rd9], %rs26;

$L__BB34_6:
	ret;

}
	// .globl	sub_gpu_half2
.visible .entry sub_gpu_half2(
	.param .u64 sub_gpu_half2_param_0,
	.param .u64 sub_gpu_half2_param_1,
	.param .u64 sub_gpu_half2_param_2,
	.param .u32 sub_gpu_half2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_half2_param_0];
	ld.param.u64 	%rd2, [sub_gpu_half2_param_1];
	ld.param.u64 	%rd3, [sub_gpu_half2_param_2];
	ld.param.u32 	%r2, [sub_gpu_half2_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB35_2;

	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s64 	%rd8, %rd4, %rd6;
	ld.global.nc.u32 	%r7, [%rd8];
	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd10, %rd9, %rd6;
	ld.global.nc.u32 	%r8, [%rd10];
	// begin inline asm
	{sub.f16x2 %r6,%r7,%r8;
}
	// end inline asm
	st.global.u32 	[%rd7], %r6;

$L__BB35_2:
	ret;

}
	// .globl	sub_gpu
.visible .entry sub_gpu(
	.param .u64 sub_gpu_param_0,
	.param .u64 sub_gpu_param_1,
	.param .u64 sub_gpu_param_2,
	.param .u32 sub_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_param_0];
	ld.param.u64 	%rd2, [sub_gpu_param_1];
	ld.param.u64 	%rd3, [sub_gpu_param_2];
	ld.param.u32 	%r2, [sub_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB36_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.f32 	%f1, [%rd8];
	ld.global.nc.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

$L__BB36_2:
	ret;

}
	// .globl	sub_gpu_half
.visible .entry sub_gpu_half(
	.param .u64 sub_gpu_half_param_0,
	.param .u64 sub_gpu_half_param_1,
	.param .u64 sub_gpu_half_param_2,
	.param .u32 sub_gpu_half_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [sub_gpu_half_param_0];
	ld.param.u64 	%rd2, [sub_gpu_half_param_1];
	ld.param.u64 	%rd3, [sub_gpu_half_param_2];
	ld.param.u32 	%r2, [sub_gpu_half_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB37_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.nc.u16 	%rs3, [%rd8];
	// begin inline asm
	{sub.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.u16 	[%rd10], %rs1;

$L__BB37_2:
	ret;

}
	// .globl	sub_half_float_gpu
.visible .entry sub_half_float_gpu(
	.param .u64 sub_half_float_gpu_param_0,
	.param .u64 sub_half_float_gpu_param_1,
	.param .u64 sub_half_float_gpu_param_2,
	.param .u32 sub_half_float_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [sub_half_float_gpu_param_0];
	ld.param.u64 	%rd2, [sub_half_float_gpu_param_1];
	ld.param.u64 	%rd3, [sub_half_float_gpu_param_2];
	ld.param.u32 	%r2, [sub_half_float_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB38_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.f32 	%f2, [%rd9];
	sub.ftz.f32 	%f3, %f1, %f2;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.f32 	[%rd11], %f3;

$L__BB38_2:
	ret;

}
	// .globl	sub_float_half_gpu
.visible .entry sub_float_half_gpu(
	.param .u64 sub_float_half_gpu_param_0,
	.param .u64 sub_float_half_gpu_param_1,
	.param .u64 sub_float_half_gpu_param_2,
	.param .u32 sub_float_half_gpu_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [sub_float_half_gpu_param_0];
	ld.param.u64 	%rd2, [sub_float_half_gpu_param_1];
	ld.param.u64 	%rd3, [sub_float_half_gpu_param_2];
	ld.param.u32 	%r2, [sub_float_half_gpu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB39_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u16 	%rs1, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	sub.ftz.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f32 	[%rd11], %f3;

$L__BB39_2:
	ret;

}
	// .globl	mul
.visible .entry mul(
	.param .u64 mul_param_0,
	.param .f32 mul_param_1,
	.param .u32 mul_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [mul_param_0];
	ld.param.f32 	%f1, [mul_param_1];
	ld.param.u32 	%r2, [mul_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB40_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	mul.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB40_2:
	ret;

}
	// .globl	mul_float
.visible .entry mul_float(
	.param .u64 mul_float_param_0,
	.param .f32 mul_float_param_1,
	.param .u32 mul_float_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [mul_float_param_0];
	ld.param.f32 	%f1, [mul_float_param_1];
	ld.param.u32 	%r2, [mul_float_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB41_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	mul.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

$L__BB41_2:
	ret;

}
	// .globl	clip
.visible .entry clip(
	.param .u64 clip_param_0,
	.param .f32 clip_param_1,
	.param .f32 clip_param_2,
	.param .u32 clip_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [clip_param_0];
	ld.param.f32 	%f2, [clip_param_1];
	ld.param.f32 	%f3, [clip_param_2];
	ld.param.u32 	%r2, [clip_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB42_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	setp.gt.ftz.f32 	%p2, %f1, %f3;
	@%p2 bra 	$L__BB42_4;
	bra.uni 	$L__BB42_2;

$L__BB42_4:
	st.global.f32 	[%rd1], %f3;
	bra.uni 	$L__BB42_5;

$L__BB42_2:
	setp.geu.ftz.f32 	%p3, %f1, %f2;
	@%p3 bra 	$L__BB42_5;

	st.global.f32 	[%rd1], %f2;

$L__BB42_5:
	ret;

}
	// .globl	clip_half
.visible .entry clip_half(
	.param .u64 clip_half_param_0,
	.param .align 2 .b8 clip_half_param_1[2],
	.param .align 2 .b8 clip_half_param_2[2],
	.param .u32 clip_half_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u16 	%rs5, [clip_half_param_2];
	ld.param.u16 	%rs4, [clip_half_param_1];
	ld.param.u64 	%rd2, [clip_half_param_0];
	ld.param.u32 	%r2, [clip_half_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB43_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd1];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs2, %rs5;
  selp.u16 %rs6, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs6, 0;
	@%p2 bra 	$L__BB43_3;

	st.global.u16 	[%rd1], %rs5;
	bra.uni 	$L__BB43_5;

$L__BB43_3:
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs2, %rs4;
  selp.u16 %rs9, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs9, 0;
	@%p3 bra 	$L__BB43_5;

	st.global.u16 	[%rd1], %rs4;

$L__BB43_5:
	ret;

}
	// .globl	pow2
.visible .entry pow2(
	.param .u64 pow2_param_0,
	.param .u32 pow2_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [pow2_param_0];
	ld.param.u32 	%r2, [pow2_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB44_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mul.ftz.f32 	%f2, %f1, %f1;
	st.global.f32 	[%rd4], %f2;

$L__BB44_2:
	ret;

}
	// .globl	pow2_half
.visible .entry pow2_half(
	.param .u64 pow2_half_param_0,
	.param .u32 pow2_half_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [pow2_half_param_0];
	ld.param.u32 	%r2, [pow2_half_param_1];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB45_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 2;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u16 	%rs2, [%rd4];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	st.global.u16 	[%rd4], %rs1;

$L__BB45_2:
	ret;

}
	// .globl	subAbs_half
.visible .entry subAbs_half(
	.param .u64 subAbs_half_param_0,
	.param .u64 subAbs_half_param_1,
	.param .u64 subAbs_half_param_2,
	.param .u32 subAbs_half_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subAbs_half_param_0];
	ld.param.u64 	%rd2, [subAbs_half_param_1];
	ld.param.u64 	%rd3, [subAbs_half_param_2];
	ld.param.u32 	%r2, [subAbs_half_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB46_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs2, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.u16 	%rs3, [%rd8];
	// begin inline asm
	{sub.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{abs.f16 %rs4,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.u16 	[%rd10], %rs4;

$L__BB46_2:
	ret;

}
	// .globl	subAbs
.visible .entry subAbs(
	.param .u64 subAbs_param_0,
	.param .u64 subAbs_param_1,
	.param .u64 subAbs_param_2,
	.param .u32 subAbs_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subAbs_param_0];
	ld.param.u64 	%rd2, [subAbs_param_1];
	ld.param.u64 	%rd3, [subAbs_param_2];
	ld.param.u32 	%r2, [subAbs_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB47_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	abs.ftz.f32 	%f4, %f3;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f4;

$L__BB47_2:
	ret;

}
	// .globl	sum
.visible .entry sum(
	.param .u64 sum_param_0,
	.param .u64 sum_param_1,
	.param .u32 sum_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [sum_param_0];
	ld.param.u64 	%rd2, [sum_param_1];
	ld.param.u32 	%r2, [sum_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB48_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	atom.global.add.f32 	%f2, [%rd6], %f1;

$L__BB48_2:
	ret;

}
	// .globl	sum_half
.visible .entry sum_half(
	.param .u64 sum_half_param_0,
	.param .u64 sum_half_param_1,
	.param .u32 sum_half_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [sum_half_param_0];
	ld.param.u64 	%rd2, [sum_half_param_1];
	ld.param.u32 	%r2, [sum_half_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB49_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs2, [%rd6];
	// begin inline asm
	{ atom.add.noftz.f16 %rs1,[%rd2],%rs2; }

	// end inline asm

$L__BB49_2:
	ret;

}
	// .globl	derAbs
.visible .entry derAbs(
	.param .u64 derAbs_param_0,
	.param .u64 derAbs_param_1,
	.param .u64 derAbs_param_2,
	.param .u32 derAbs_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [derAbs_param_0];
	ld.param.u64 	%rd2, [derAbs_param_1];
	ld.param.u64 	%rd3, [derAbs_param_2];
	ld.param.u32 	%r2, [derAbs_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB50_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	abs.ftz.f32 	%f4, %f3;
	div.approx.ftz.f32 	%f5, %f3, %f4;
	add.ftz.f32 	%f6, %f5, 0f33D6BF95;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f6;

$L__BB50_2:
	ret;

}
	// .globl	fisnan
.visible .entry fisnan(
	.param .u64 fisnan_param_0,
	.param .u64 fisnan_param_1,
	.param .u32 fisnan_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_param_0];
	ld.param.u64 	%rd3, [fisnan_param_1];
	ld.param.u32 	%r2, [fisnan_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB51_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB51_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs2, [%rd6];
	// begin inline asm
	{set.nan.f16.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	setp.eq.s16 	%p3, %rs1, 0;
	@%p3 bra 	$L__BB51_4;

	st.global.u32 	[%rd1], %r1;

$L__BB51_4:
	ret;

}
	// .globl	fisnan_float
.visible .entry fisnan_float(
	.param .u64 fisnan_float_param_0,
	.param .u64 fisnan_float_param_1,
	.param .u32 fisnan_float_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [fisnan_float_param_0];
	ld.param.u64 	%rd3, [fisnan_float_param_1];
	ld.param.u32 	%r2, [fisnan_float_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB52_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB52_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f1;
	abs.f64 	%fd2, %fd1;
	setp.le.f64 	%p3, %fd2, 0d7FF0000000000000;
	@%p3 bra 	$L__BB52_4;

	st.global.u32 	[%rd1], %r1;

$L__BB52_4:
	ret;

}
	// .globl	hisinf
.visible .entry hisinf(
	.param .u64 hisinf_param_0,
	.param .u64 hisinf_param_1,
	.param .u32 hisinf_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_param_0];
	ld.param.u64 	%rd3, [hisinf_param_1];
	ld.param.u32 	%r2, [hisinf_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB53_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB53_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6];
	or.b16  	%rs2, %rs1, -32768;
	setp.ne.s16 	%p3, %rs2, -1024;
	setp.ne.s16 	%p4, %rs1, -1024;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB53_4;

	st.global.u32 	[%rd1], %r1;

$L__BB53_4:
	ret;

}
	// .globl	hisinf_float
.visible .entry hisinf_float(
	.param .u64 hisinf_float_param_0,
	.param .u64 hisinf_float_param_1,
	.param .u32 hisinf_float_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [hisinf_float_param_0];
	ld.param.u64 	%rd3, [hisinf_float_param_1];
	ld.param.u32 	%r2, [hisinf_float_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB54_4;

	ld.global.u32 	%r6, [%rd1];
	setp.ne.s32 	%p2, %r6, 0;
	@%p2 bra 	$L__BB54_4;

	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	cvt.ftz.f64.f32 	%fd1, %f1;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd1;
	}
	and.b32  	%r9, %r8, 2147483647;
	setp.ne.s32 	%p3, %r9, 2146435072;
	setp.ne.s32 	%p4, %r7, 0;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB54_4;

	st.global.u32 	[%rd1], %r1;

$L__BB54_4:
	ret;

}
	// .globl	momentum
.visible .entry momentum(
	.param .u64 momentum_param_0,
	.param .u64 momentum_param_1,
	.param .align 2 .b8 momentum_param_2[2],
	.param .u32 momentum_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<14>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs1, [momentum_param_2];
	ld.param.u64 	%rd1, [momentum_param_0];
	ld.param.u64 	%rd2, [momentum_param_1];
	ld.param.u32 	%r2, [momentum_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB55_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// begin inline asm
	{mul.f16 %rs2,%rs1,%rs4;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+8];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs9, [%rd7];
	// begin inline asm
	{mul.f16 %rs8,%rs9,%rs5;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs11,%rs2,%rs8;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs11;

$L__BB55_2:
	ret;

}
	// .globl	momentum_float
.visible .entry momentum_float(
	.param .u64 momentum_float_param_0,
	.param .u64 momentum_float_param_1,
	.param .f32 momentum_float_param_2,
	.param .u32 momentum_float_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [momentum_float_param_0];
	ld.param.u64 	%rd2, [momentum_float_param_1];
	ld.param.f32 	%f1, [momentum_float_param_2];
	ld.param.u32 	%r2, [momentum_float_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB56_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	mov.f32 	%f3, 0f3F800000;
	sub.ftz.f32 	%f4, %f3, %f1;
	ld.global.nc.f32 	%f5, [%rd7];
	mul.ftz.f32 	%f6, %f4, %f5;
	fma.rn.ftz.f32 	%f7, %f2, %f1, %f6;
	st.global.f32 	[%rd5], %f7;

$L__BB56_2:
	ret;

}
	// .globl	momentumPow2
.visible .entry momentumPow2(
	.param .u64 momentumPow2_param_0,
	.param .u64 momentumPow2_param_1,
	.param .align 2 .b8 momentumPow2_param_2[2],
	.param .u32 momentumPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u16 	%rs1, [momentumPow2_param_2];
	ld.param.u64 	%rd1, [momentumPow2_param_0];
	ld.param.u64 	%rd2, [momentumPow2_param_1];
	ld.param.u32 	%r2, [momentumPow2_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB57_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// begin inline asm
	{mul.f16 %rs2,%rs1,%rs4;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+8];
	// begin inline asm
	{sub.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.u16 	%rs10, [%rd7];
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs10;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs11,%rs8,%rs10;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	st.global.u16 	[%rd5], %rs14;

$L__BB57_2:
	ret;

}
	// .globl	momentumPow2_float
.visible .entry momentumPow2_float(
	.param .u64 momentumPow2_float_param_0,
	.param .u64 momentumPow2_float_param_1,
	.param .f32 momentumPow2_float_param_2,
	.param .u32 momentumPow2_float_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [momentumPow2_float_param_0];
	ld.param.u64 	%rd2, [momentumPow2_float_param_1];
	ld.param.f32 	%f1, [momentumPow2_float_param_2];
	ld.param.u32 	%r2, [momentumPow2_float_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB58_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.nc.f32 	%f3, [%rd7];
	mov.f32 	%f4, 0f3F800000;
	sub.ftz.f32 	%f5, %f4, %f1;
	mul.ftz.f32 	%f6, %f5, %f3;
	mul.ftz.f32 	%f7, %f3, %f6;
	fma.rn.ftz.f32 	%f8, %f2, %f1, %f7;
	st.global.f32 	[%rd5], %f8;

$L__BB58_2:
	ret;

}
	// .globl	subDivSqrtNorm_half
.visible .entry subDivSqrtNorm_half(
	.param .u64 subDivSqrtNorm_half_param_0,
	.param .u64 subDivSqrtNorm_half_param_1,
	.param .align 2 .b8 subDivSqrtNorm_half_param_2[2],
	.param .align 2 .b8 subDivSqrtNorm_half_param_3[2],
	.param .align 2 .b8 subDivSqrtNorm_half_param_4[2],
	.param .u64 subDivSqrtNorm_half_param_5,
	.param .u32 subDivSqrtNorm_half_param_6
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<75>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u16 	%rs17, [subDivSqrtNorm_half_param_4];
	ld.param.u16 	%rs16, [subDivSqrtNorm_half_param_3];
	ld.param.u16 	%rs15, [subDivSqrtNorm_half_param_2];
	ld.param.u64 	%rd2, [subDivSqrtNorm_half_param_0];
	ld.param.u64 	%rd3, [subDivSqrtNorm_half_param_1];
	ld.param.u64 	%rd4, [subDivSqrtNorm_half_param_5];
	ld.param.u32 	%r2, [subDivSqrtNorm_half_param_6];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB59_11;

	ld.const.u16 	%rs1, [sh+10];
	// begin inline asm
	{add.f16 %rs18,%rs16,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs15;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs18;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f15, %f14;
}
	// end inline asm
	mul.ftz.f32 	%f17, %f13, %f15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f17;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs24,%rs72;
}
	// end inline asm
	mov.u16 	%rs28, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs24, %rs28;
  selp.u16 %rs26, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs26, 0;
	@%p2 bra 	$L__BB59_4;

	mov.f32 	%f18, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f18;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs29, %rs24;
  selp.u16 %rs30, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs30, 0;
	@%p3 bra 	$L__BB59_4;

	neg.ftz.f32 	%f20, %f14;
	fma.rn.ftz.f32 	%f21, %f20, %f17, %f13;
	fma.rn.ftz.f32 	%f19, %f15, %f21, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f19;}

	// end inline asm

$L__BB59_4:
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd1, %rd4, %rd5;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.nc.u16 	%rs36, [%rd7];
	// begin inline asm
	{mul.f16 %rs34,%rs72,%rs36;
}
	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd5;
	ld.global.nc.u16 	%rs37, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs17;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f24, %f23;
}
	// end inline asm
	mul.ftz.f32 	%f26, %f22, %f24;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f26;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs40,%rs73;
}
	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs40, %rs28;
  selp.u16 %rs42, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs42, 0;
	@%p4 bra 	$L__BB59_7;

	mov.f32 	%f27, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f27;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs45, %rs40;
  selp.u16 %rs46, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs46, 0;
	@%p5 bra 	$L__BB59_7;

	neg.ftz.f32 	%f29, %f23;
	fma.rn.ftz.f32 	%f30, %f29, %f26, %f22;
	fma.rn.ftz.f32 	%f28, %f24, %f30, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f28;}

	// end inline asm

$L__BB59_7:
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs73;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs50,r;     
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs52,%rs50,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs52;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f33, %f32;
}
	// end inline asm
	mul.ftz.f32 	%f35, %f31, %f33;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f35;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs58,%rs74;
}
	// end inline asm
	mov.u16 	%rs62, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs58, %rs62;
  selp.u16 %rs60, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs60, 0;
	@%p6 bra 	$L__BB59_10;

	mov.f32 	%f36, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f36;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs63, %rs58;
  selp.u16 %rs64, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs64, 0;
	@%p7 bra 	$L__BB59_10;

	neg.ftz.f32 	%f38, %f32;
	fma.rn.ftz.f32 	%f39, %f38, %f35, %f31;
	fma.rn.ftz.f32 	%f37, %f33, %f39, %f35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f37;}

	// end inline asm

$L__BB59_10:
	// begin inline asm
	{neg.f16 %rs68,%rs74;
}
	// end inline asm
	// begin inline asm
	{ atom.add.noftz.f16 %rs70,[%rd1],%rs68; }

	// end inline asm

$L__BB59_11:
	ret;

}
	// .globl	subDivSqrtNorm
.visible .entry subDivSqrtNorm(
	.param .u64 subDivSqrtNorm_param_0,
	.param .u64 subDivSqrtNorm_param_1,
	.param .f32 subDivSqrtNorm_param_2,
	.param .f32 subDivSqrtNorm_param_3,
	.param .f32 subDivSqrtNorm_param_4,
	.param .u64 subDivSqrtNorm_param_5,
	.param .u32 subDivSqrtNorm_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [subDivSqrtNorm_param_0];
	ld.param.u64 	%rd2, [subDivSqrtNorm_param_1];
	ld.param.f32 	%f1, [subDivSqrtNorm_param_2];
	ld.param.f32 	%f2, [subDivSqrtNorm_param_3];
	ld.param.f32 	%f3, [subDivSqrtNorm_param_4];
	ld.param.u64 	%rd3, [subDivSqrtNorm_param_5];
	ld.param.u32 	%r2, [subDivSqrtNorm_param_6];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB60_2;

	cvta.to.global.u64 	%rd4, %rd1;
	add.ftz.f32 	%f4, %f2, 0f33D6BF95;
	div.approx.ftz.f32 	%f5, %f1, %f4;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s64 	%rd8, %rd4, %rd6;
	ld.global.nc.f32 	%f6, [%rd8];
	mul.ftz.f32 	%f7, %f5, %f6;
	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd10, %rd9, %rd6;
	ld.global.nc.f32 	%f8, [%rd10];
	div.approx.ftz.f32 	%f9, %f8, %f3;
	sqrt.approx.ftz.f32 	%f10, %f9;
	add.ftz.f32 	%f11, %f10, 0f358637BD;
	div.approx.ftz.f32 	%f12, %f7, %f11;
	neg.ftz.f32 	%f13, %f12;
	atom.global.add.f32 	%f14, [%rd7], %f13;

$L__BB60_2:
	ret;

}
	// .globl	addBackCopy
.visible .entry addBackCopy(
	.param .u64 addBackCopy_param_0,
	.param .u32 addBackCopy_param_1,
	.param .u32 addBackCopy_param_2,
	.param .u32 addBackCopy_param_3,
	.param .u32 addBackCopy_param_4,
	.param .u64 addBackCopy_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_param_0];
	ld.param.u32 	%r3, [addBackCopy_param_1];
	ld.param.u32 	%r6, [addBackCopy_param_2];
	ld.param.u32 	%r4, [addBackCopy_param_3];
	ld.param.u32 	%r5, [addBackCopy_param_4];
	ld.param.u64 	%rd2, [addBackCopy_param_5];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB61_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r14, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB61_2:
	ret;

}
	// .globl	addBackCopy_half
.visible .entry addBackCopy_half(
	.param .u64 addBackCopy_half_param_0,
	.param .u32 addBackCopy_half_param_1,
	.param .u32 addBackCopy_half_param_2,
	.param .u32 addBackCopy_half_param_3,
	.param .u32 addBackCopy_half_param_4,
	.param .u64 addBackCopy_half_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addBackCopy_half_param_0];
	ld.param.u32 	%r3, [addBackCopy_half_param_1];
	ld.param.u32 	%r6, [addBackCopy_half_param_2];
	ld.param.u32 	%r4, [addBackCopy_half_param_3];
	ld.param.u32 	%r5, [addBackCopy_half_param_4];
	ld.param.u64 	%rd2, [addBackCopy_half_param_5];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB62_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r1, %r3, %r2;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	mad.lo.s32 	%r15, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r14, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.u16 	%rs1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r15, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB62_2:
	ret;

}
	// .globl	dropoutBack
.visible .entry dropoutBack(
	.param .u64 dropoutBack_param_0,
	.param .u64 dropoutBack_param_1,
	.param .align 2 .b8 dropoutBack_param_2[2],
	.param .u64 dropoutBack_param_3,
	.param .u32 dropoutBack_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<13>;


	ld.param.u16 	%rs1, [dropoutBack_param_2];
	ld.param.u64 	%rd2, [dropoutBack_param_0];
	ld.param.u64 	%rd3, [dropoutBack_param_1];
	ld.param.u64 	%rd4, [dropoutBack_param_3];
	ld.param.u32 	%r2, [dropoutBack_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB63_3;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs3, [%rd7];
	ld.const.u16 	%rs4, [sh];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.neu.f16  __$temp3, %rs3, %rs4;
  selp.u16 %rs2, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs2, 0;
	@%p2 bra 	$L__BB63_3;

	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs6, [%rd10];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.u16 	[%rd12], %rs5;

$L__BB63_3:
	ret;

}
	// .globl	mask
.visible .entry mask(
	.param .u64 mask_param_0,
	.param .f32 mask_param_1,
	.param .f32 mask_param_2,
	.param .u64 mask_param_3,
	.param .u32 mask_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [mask_param_0];
	ld.param.f32 	%f1, [mask_param_1];
	ld.param.f32 	%f2, [mask_param_2];
	ld.param.u64 	%rd3, [mask_param_3];
	ld.param.u32 	%r2, [mask_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB64_3;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f3, [%rd6];
	setp.neu.ftz.f32 	%p2, %f3, %f1;
	@%p2 bra 	$L__BB64_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f2;

$L__BB64_3:
	ret;

}
	// .globl	mask_half
.visible .entry mask_half(
	.param .u64 mask_half_param_0,
	.param .align 2 .b8 mask_half_param_1[2],
	.param .align 2 .b8 mask_half_param_2[2],
	.param .u64 mask_half_param_3,
	.param .u32 mask_half_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u16 	%rs2, [mask_half_param_2];
	ld.param.u16 	%rs1, [mask_half_param_1];
	ld.param.u64 	%rd2, [mask_half_param_0];
	ld.param.u64 	%rd3, [mask_half_param_3];
	ld.param.u32 	%r2, [mask_half_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB65_3;

	cvt.s64.s32 	%rd1, %r1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs4, [%rd6];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.eq.f16  __$temp3, %rs4, %rs1;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs3, 0;
	@%p2 bra 	$L__BB65_3;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB65_3:
	ret;

}
	// .globl	fillUnderDiagonal
.visible .entry fillUnderDiagonal(
	.param .u32 fillUnderDiagonal_param_0,
	.param .f32 fillUnderDiagonal_param_1,
	.param .u64 fillUnderDiagonal_param_2,
	.param .u32 fillUnderDiagonal_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r11, [fillUnderDiagonal_param_0];
	ld.param.f32 	%f1, [fillUnderDiagonal_param_1];
	ld.param.u64 	%rd8, [fillUnderDiagonal_param_2];
	ld.param.u32 	%r12, [fillUnderDiagonal_param_3];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	setp.lt.s32 	%p2, %r1, 0;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB66_7;

	add.s32 	%r17, %r1, 1;
	and.b32  	%r24, %r17, 3;
	setp.lt.u32 	%p4, %r1, 3;
	mov.u32 	%r23, 0;
	@%p4 bra 	$L__BB66_4;

	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd9, %r19, 4;
	add.s64 	%rd10, %rd1, %rd9;
	add.s64 	%rd12, %rd10, 8;
	sub.s32 	%r21, %r1, %r24;

$L__BB66_3:
	st.global.f32 	[%rd12+-8], %f1;
	st.global.f32 	[%rd12+-4], %f1;
	st.global.f32 	[%rd12], %f1;
	st.global.f32 	[%rd12+4], %f1;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd12, %rd12, 16;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p5, %r21, -1;
	@%p5 bra 	$L__BB66_3;

$L__BB66_4:
	setp.eq.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB66_7;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd11, %r20, 4;
	add.s64 	%rd13, %rd1, %rd11;

$L__BB66_6:
	.pragma "nounroll";
	st.global.f32 	[%rd13], %f1;
	add.s64 	%rd13, %rd13, 4;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p7, %r24, 0;
	@%p7 bra 	$L__BB66_6;

$L__BB66_7:
	ret;

}
	// .globl	fillUnderDiagonal_half
.visible .entry fillUnderDiagonal_half(
	.param .u32 fillUnderDiagonal_half_param_0,
	.param .align 2 .b8 fillUnderDiagonal_half_param_1[2],
	.param .u64 fillUnderDiagonal_half_param_2,
	.param .u32 fillUnderDiagonal_half_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<14>;


	ld.param.u16 	%rs2, [fillUnderDiagonal_half_param_1];
	ld.param.u32 	%r11, [fillUnderDiagonal_half_param_0];
	ld.param.u64 	%rd8, [fillUnderDiagonal_half_param_2];
	ld.param.u32 	%r12, [fillUnderDiagonal_half_param_3];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	setp.lt.s32 	%p2, %r1, 0;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB67_7;

	add.s32 	%r17, %r1, 1;
	and.b32  	%r24, %r17, 3;
	setp.lt.u32 	%p4, %r1, 3;
	mov.u32 	%r23, 0;
	@%p4 bra 	$L__BB67_4;

	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd9, %r19, 2;
	add.s64 	%rd10, %rd1, %rd9;
	add.s64 	%rd12, %rd10, 4;
	sub.s32 	%r21, %r1, %r24;

$L__BB67_3:
	st.global.u16 	[%rd12+-4], %rs2;
	st.global.u16 	[%rd12+-2], %rs2;
	st.global.u16 	[%rd12], %rs2;
	st.global.u16 	[%rd12+2], %rs2;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd12, %rd12, 8;
	add.s32 	%r21, %r21, -4;
	setp.ne.s32 	%p5, %r21, -1;
	@%p5 bra 	$L__BB67_3;

$L__BB67_4:
	setp.eq.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB67_7;

	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd11, %r20, 2;
	add.s64 	%rd13, %rd1, %rd11;

$L__BB67_6:
	.pragma "nounroll";
	st.global.u16 	[%rd13], %rs2;
	add.s64 	%rd13, %rd13, 2;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p7, %r24, 0;
	@%p7 bra 	$L__BB67_6;

$L__BB67_7:
	ret;

}
	// .globl	derGelu
.visible .entry derGelu(
	.param .u64 derGelu_param_0,
	.param .u64 derGelu_param_1,
	.param .u64 derGelu_param_2,
	.param .u32 derGelu_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [derGelu_param_0];
	ld.param.u64 	%rd3, [derGelu_param_1];
	ld.param.u64 	%rd4, [derGelu_param_2];
	ld.param.u32 	%r2, [derGelu_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB68_5;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.f32 	%f1, [%rd7];
	mul.ftz.f32 	%f7, %f1, 0f3D122277;
	mul.ftz.f32 	%f8, %f1, %f7;
	mul.ftz.f32 	%f9, %f1, %f8;
	fma.rn.ftz.f32 	%f2, %f1, 0f3F4C422A, %f9;
	abs.ftz.f32 	%f3, %f2;
	setp.ltu.ftz.f32 	%p2, %f3, 0f3F19999A;
	@%p2 bra 	$L__BB68_3;
	bra.uni 	$L__BB68_2;

$L__BB68_3:
	mul.ftz.f32 	%f18, %f2, %f2;
	mov.f32 	%f19, 0fBD563CAE;
	mov.f32 	%f20, 0f3C80F082;
	fma.rn.ftz.f32 	%f21, %f20, %f18, %f19;
	mov.f32 	%f22, 0f3E085941;
	fma.rn.ftz.f32 	%f23, %f21, %f18, %f22;
	mov.f32 	%f24, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f25, %f23, %f18, %f24;
	mov.f32 	%f26, 0f00000000;
	fma.rn.ftz.f32 	%f27, %f25, %f18, %f26;
	fma.rn.ftz.f32 	%f39, %f27, %f2, %f2;
	bra.uni 	$L__BB68_4;

$L__BB68_2:
	mul.ftz.f32 	%f10, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f11, %f10;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f3F800000;
	rcp.approx.ftz.f32 	%f14, %f12;
	mov.f32 	%f15, 0fC0000000;
	fma.rn.ftz.f32 	%f16, %f14, %f15, %f13;
	setp.ge.ftz.f32 	%p3, %f3, 0f41102CB4;
	selp.f32 	%f17, 0f3F800000, %f16, %p3;
	mov.b32 	%r6, %f17;
	mov.b32 	%r7, %f2;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f39, %r9;

$L__BB68_4:
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.f32 	%f28, [%rd10];
	mul.ftz.f32 	%f29, %f28, 0f3F000000;
	mul.ftz.f32 	%f30, %f39, %f39;
	mov.f32 	%f31, 0f3F800000;
	sub.ftz.f32 	%f32, %f31, %f30;
	mul.ftz.f32 	%f33, %f1, %f32;
	mul.ftz.f32 	%f34, %f1, 0f3DDB33B3;
	fma.rn.ftz.f32 	%f35, %f1, %f34, 0f3F4C426B;
	add.ftz.f32 	%f36, %f39, 0f3F800000;
	fma.rn.ftz.f32 	%f37, %f35, %f33, %f36;
	mul.ftz.f32 	%f38, %f29, %f37;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.f32 	[%rd12], %f38;

$L__BB68_5:
	ret;

}
	// .globl	derGelu_half
.visible .entry derGelu_half(
	.param .u64 derGelu_half_param_0,
	.param .u64 derGelu_half_param_1,
	.param .u64 derGelu_half_param_2,
	.param .u32 derGelu_half_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<52>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [derGelu_half_param_0];
	ld.param.u64 	%rd3, [derGelu_half_param_1];
	ld.param.u64 	%rd4, [derGelu_half_param_2];
	ld.param.u32 	%r2, [derGelu_half_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB69_5;

	cvta.to.global.u64 	%rd5, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	ld.const.u16 	%rs3, [sh+2];
	// begin inline asm
	{mul.f16 %rs2,%rs3,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs6, [sh+4];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs8,%rs5,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs11,%rs8,%rs1;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs2,%rs11;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs14;}

	// end inline asm
	abs.ftz.f32 	%f2, %f6;
	setp.ltu.ftz.f32 	%p2, %f2, 0f3F19999A;
	@%p2 bra 	$L__BB69_3;
	bra.uni 	$L__BB69_2;

$L__BB69_3:
	mul.ftz.f32 	%f15, %f6, %f6;
	mov.f32 	%f16, 0fBD563CAE;
	mov.f32 	%f17, 0f3C80F082;
	fma.rn.ftz.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0f3E085941;
	fma.rn.ftz.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0fBEAAA9ED;
	fma.rn.ftz.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f00000000;
	fma.rn.ftz.f32 	%f24, %f22, %f15, %f23;
	fma.rn.ftz.f32 	%f26, %f24, %f6, %f6;
	bra.uni 	$L__BB69_4;

$L__BB69_2:
	mul.ftz.f32 	%f7, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f8, %f7;
	add.ftz.f32 	%f9, %f8, 0f3F800000;
	mov.f32 	%f10, 0f3F800000;
	rcp.approx.ftz.f32 	%f11, %f9;
	mov.f32 	%f12, 0fC0000000;
	fma.rn.ftz.f32 	%f13, %f11, %f12, %f10;
	setp.ge.ftz.f32 	%p3, %f2, 0f41102CB4;
	selp.f32 	%f14, 0f3F800000, %f13, %p3;
	mov.b32 	%r6, %f14;
	mov.b32 	%r7, %f6;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f26, %r9;

$L__BB69_4:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f26;}

	// end inline asm
	cvta.to.global.u64 	%rd8, %rd3;
	shl.b64 	%rd9, %rd1, 1;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u16 	%rs20, [%rd10];
	ld.const.u16 	%rs21, [sh+6];
	// begin inline asm
	{mul.f16 %rs19,%rs20,%rs21;
}
	// end inline asm
	ld.const.u16 	%rs23, [sh+8];
	// begin inline asm
	{add.f16 %rs22,%rs23,%rs18;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs25,%rs18,%rs18;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs28,%rs23,%rs25;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs31,%rs1,%rs28;
}
	// end inline asm
	ld.const.u16 	%rs35, [sh+22];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs1;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs37,%rs34,%rs1;
}
	// end inline asm
	ld.const.u16 	%rs41, [sh+20];
	// begin inline asm
	{add.f16 %rs40,%rs41,%rs37;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs43,%rs31,%rs40;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs46,%rs22,%rs43;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs49,%rs19,%rs46;
}
	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd9;
	st.global.u16 	[%rd12], %rs49;

$L__BB69_5:
	ret;

}
	// .globl	matvec_kernel
.visible .entry matvec_kernel(
	.param .u64 matvec_kernel_param_0,
	.param .u64 matvec_kernel_param_1,
	.param .u64 matvec_kernel_param_2,
	.param .u32 matvec_kernel_param_3,
	.param .u32 matvec_kernel_param_4
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<75>;
	// demoted variable
	.shared .align 4 .b8 _ZZ13matvec_kernelE8x_shared[128];

	ld.param.u64 	%rd3, [matvec_kernel_param_0];
	ld.param.u64 	%rd4, [matvec_kernel_param_1];
	ld.param.u64 	%rd5, [matvec_kernel_param_2];
	ld.param.u32 	%r13, [matvec_kernel_param_3];
	ld.param.u32 	%r14, [matvec_kernel_param_4];
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r51, %tid.x;
	mad.lo.s32 	%r2, %r16, %r15, %r51;
	add.s32 	%r17, %r14, 31;
	shr.u32 	%r3, %r17, 5;
	setp.eq.s32 	%p1, %r3, 0;
	mov.f32 	%f106, 0f00000000;
	@%p1 bra 	$L__BB70_5;

	cvta.to.global.u64 	%rd1, %rd3;
	shl.b32 	%r18, %r51, 2;
	mov.u32 	%r19, _ZZ13matvec_kernelE8x_shared;
	add.s32 	%r4, %r19, %r18;
	mul.lo.s32 	%r52, %r2, %r14;
	neg.s32 	%r53, %r3;
	cvta.to.global.u64 	%rd2, %rd4;

$L__BB70_2:
	setp.ge.u32 	%p2, %r51, %r14;
	mov.f32 	%f105, 0f00000000;
	@%p2 bra 	$L__BB70_4;

	mul.wide.u32 	%rd6, %r51, 4;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.nc.f32 	%f105, [%rd7];

$L__BB70_4:
	st.shared.f32 	[%r4], %f105;
	bar.sync 	0;
	mul.wide.u32 	%rd8, %r52, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.shared.f32 	%f9, [_ZZ13matvec_kernelE8x_shared];
	ld.global.nc.f32 	%f10, [%rd9];
	fma.rn.ftz.f32 	%f11, %f10, %f9, %f106;
	add.s32 	%r20, %r52, 1;
	mul.wide.u32 	%rd10, %r20, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.shared.f32 	%f12, [_ZZ13matvec_kernelE8x_shared+4];
	ld.global.nc.f32 	%f13, [%rd11];
	fma.rn.ftz.f32 	%f14, %f13, %f12, %f11;
	add.s32 	%r21, %r52, 2;
	mul.wide.u32 	%rd12, %r21, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.shared.f32 	%f15, [_ZZ13matvec_kernelE8x_shared+8];
	ld.global.nc.f32 	%f16, [%rd13];
	fma.rn.ftz.f32 	%f17, %f16, %f15, %f14;
	add.s32 	%r22, %r52, 3;
	mul.wide.u32 	%rd14, %r22, 4;
	add.s64 	%rd15, %rd1, %rd14;
	ld.shared.f32 	%f18, [_ZZ13matvec_kernelE8x_shared+12];
	ld.global.nc.f32 	%f19, [%rd15];
	fma.rn.ftz.f32 	%f20, %f19, %f18, %f17;
	add.s32 	%r23, %r52, 4;
	mul.wide.u32 	%rd16, %r23, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.shared.f32 	%f21, [_ZZ13matvec_kernelE8x_shared+16];
	ld.global.nc.f32 	%f22, [%rd17];
	fma.rn.ftz.f32 	%f23, %f22, %f21, %f20;
	add.s32 	%r24, %r52, 5;
	mul.wide.u32 	%rd18, %r24, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.shared.f32 	%f24, [_ZZ13matvec_kernelE8x_shared+20];
	ld.global.nc.f32 	%f25, [%rd19];
	fma.rn.ftz.f32 	%f26, %f25, %f24, %f23;
	add.s32 	%r25, %r52, 6;
	mul.wide.u32 	%rd20, %r25, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.shared.f32 	%f27, [_ZZ13matvec_kernelE8x_shared+24];
	ld.global.nc.f32 	%f28, [%rd21];
	fma.rn.ftz.f32 	%f29, %f28, %f27, %f26;
	add.s32 	%r26, %r52, 7;
	mul.wide.u32 	%rd22, %r26, 4;
	add.s64 	%rd23, %rd1, %rd22;
	ld.shared.f32 	%f30, [_ZZ13matvec_kernelE8x_shared+28];
	ld.global.nc.f32 	%f31, [%rd23];
	fma.rn.ftz.f32 	%f32, %f31, %f30, %f29;
	add.s32 	%r27, %r52, 8;
	mul.wide.u32 	%rd24, %r27, 4;
	add.s64 	%rd25, %rd1, %rd24;
	ld.shared.f32 	%f33, [_ZZ13matvec_kernelE8x_shared+32];
	ld.global.nc.f32 	%f34, [%rd25];
	fma.rn.ftz.f32 	%f35, %f34, %f33, %f32;
	add.s32 	%r28, %r52, 9;
	mul.wide.u32 	%rd26, %r28, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.shared.f32 	%f36, [_ZZ13matvec_kernelE8x_shared+36];
	ld.global.nc.f32 	%f37, [%rd27];
	fma.rn.ftz.f32 	%f38, %f37, %f36, %f35;
	add.s32 	%r29, %r52, 10;
	mul.wide.u32 	%rd28, %r29, 4;
	add.s64 	%rd29, %rd1, %rd28;
	ld.shared.f32 	%f39, [_ZZ13matvec_kernelE8x_shared+40];
	ld.global.nc.f32 	%f40, [%rd29];
	fma.rn.ftz.f32 	%f41, %f40, %f39, %f38;
	add.s32 	%r30, %r52, 11;
	mul.wide.u32 	%rd30, %r30, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.shared.f32 	%f42, [_ZZ13matvec_kernelE8x_shared+44];
	ld.global.nc.f32 	%f43, [%rd31];
	fma.rn.ftz.f32 	%f44, %f43, %f42, %f41;
	add.s32 	%r31, %r52, 12;
	mul.wide.u32 	%rd32, %r31, 4;
	add.s64 	%rd33, %rd1, %rd32;
	ld.shared.f32 	%f45, [_ZZ13matvec_kernelE8x_shared+48];
	ld.global.nc.f32 	%f46, [%rd33];
	fma.rn.ftz.f32 	%f47, %f46, %f45, %f44;
	add.s32 	%r32, %r52, 13;
	mul.wide.u32 	%rd34, %r32, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.shared.f32 	%f48, [_ZZ13matvec_kernelE8x_shared+52];
	ld.global.nc.f32 	%f49, [%rd35];
	fma.rn.ftz.f32 	%f50, %f49, %f48, %f47;
	add.s32 	%r33, %r52, 14;
	mul.wide.u32 	%rd36, %r33, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.shared.f32 	%f51, [_ZZ13matvec_kernelE8x_shared+56];
	ld.global.nc.f32 	%f52, [%rd37];
	fma.rn.ftz.f32 	%f53, %f52, %f51, %f50;
	add.s32 	%r34, %r52, 15;
	mul.wide.u32 	%rd38, %r34, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.shared.f32 	%f54, [_ZZ13matvec_kernelE8x_shared+60];
	ld.global.nc.f32 	%f55, [%rd39];
	fma.rn.ftz.f32 	%f56, %f55, %f54, %f53;
	add.s32 	%r35, %r52, 16;
	mul.wide.u32 	%rd40, %r35, 4;
	add.s64 	%rd41, %rd1, %rd40;
	ld.shared.f32 	%f57, [_ZZ13matvec_kernelE8x_shared+64];
	ld.global.nc.f32 	%f58, [%rd41];
	fma.rn.ftz.f32 	%f59, %f58, %f57, %f56;
	add.s32 	%r36, %r52, 17;
	mul.wide.u32 	%rd42, %r36, 4;
	add.s64 	%rd43, %rd1, %rd42;
	ld.shared.f32 	%f60, [_ZZ13matvec_kernelE8x_shared+68];
	ld.global.nc.f32 	%f61, [%rd43];
	fma.rn.ftz.f32 	%f62, %f61, %f60, %f59;
	add.s32 	%r37, %r52, 18;
	mul.wide.u32 	%rd44, %r37, 4;
	add.s64 	%rd45, %rd1, %rd44;
	ld.shared.f32 	%f63, [_ZZ13matvec_kernelE8x_shared+72];
	ld.global.nc.f32 	%f64, [%rd45];
	fma.rn.ftz.f32 	%f65, %f64, %f63, %f62;
	add.s32 	%r38, %r52, 19;
	mul.wide.u32 	%rd46, %r38, 4;
	add.s64 	%rd47, %rd1, %rd46;
	ld.shared.f32 	%f66, [_ZZ13matvec_kernelE8x_shared+76];
	ld.global.nc.f32 	%f67, [%rd47];
	fma.rn.ftz.f32 	%f68, %f67, %f66, %f65;
	add.s32 	%r39, %r52, 20;
	mul.wide.u32 	%rd48, %r39, 4;
	add.s64 	%rd49, %rd1, %rd48;
	ld.shared.f32 	%f69, [_ZZ13matvec_kernelE8x_shared+80];
	ld.global.nc.f32 	%f70, [%rd49];
	fma.rn.ftz.f32 	%f71, %f70, %f69, %f68;
	add.s32 	%r40, %r52, 21;
	mul.wide.u32 	%rd50, %r40, 4;
	add.s64 	%rd51, %rd1, %rd50;
	ld.shared.f32 	%f72, [_ZZ13matvec_kernelE8x_shared+84];
	ld.global.nc.f32 	%f73, [%rd51];
	fma.rn.ftz.f32 	%f74, %f73, %f72, %f71;
	add.s32 	%r41, %r52, 22;
	mul.wide.u32 	%rd52, %r41, 4;
	add.s64 	%rd53, %rd1, %rd52;
	ld.shared.f32 	%f75, [_ZZ13matvec_kernelE8x_shared+88];
	ld.global.nc.f32 	%f76, [%rd53];
	fma.rn.ftz.f32 	%f77, %f76, %f75, %f74;
	add.s32 	%r42, %r52, 23;
	mul.wide.u32 	%rd54, %r42, 4;
	add.s64 	%rd55, %rd1, %rd54;
	ld.shared.f32 	%f78, [_ZZ13matvec_kernelE8x_shared+92];
	ld.global.nc.f32 	%f79, [%rd55];
	fma.rn.ftz.f32 	%f80, %f79, %f78, %f77;
	add.s32 	%r43, %r52, 24;
	mul.wide.u32 	%rd56, %r43, 4;
	add.s64 	%rd57, %rd1, %rd56;
	ld.shared.f32 	%f81, [_ZZ13matvec_kernelE8x_shared+96];
	ld.global.nc.f32 	%f82, [%rd57];
	fma.rn.ftz.f32 	%f83, %f82, %f81, %f80;
	add.s32 	%r44, %r52, 25;
	mul.wide.u32 	%rd58, %r44, 4;
	add.s64 	%rd59, %rd1, %rd58;
	ld.shared.f32 	%f84, [_ZZ13matvec_kernelE8x_shared+100];
	ld.global.nc.f32 	%f85, [%rd59];
	fma.rn.ftz.f32 	%f86, %f85, %f84, %f83;
	add.s32 	%r45, %r52, 26;
	mul.wide.u32 	%rd60, %r45, 4;
	add.s64 	%rd61, %rd1, %rd60;
	ld.shared.f32 	%f87, [_ZZ13matvec_kernelE8x_shared+104];
	ld.global.nc.f32 	%f88, [%rd61];
	fma.rn.ftz.f32 	%f89, %f88, %f87, %f86;
	add.s32 	%r46, %r52, 27;
	mul.wide.u32 	%rd62, %r46, 4;
	add.s64 	%rd63, %rd1, %rd62;
	ld.shared.f32 	%f90, [_ZZ13matvec_kernelE8x_shared+108];
	ld.global.nc.f32 	%f91, [%rd63];
	fma.rn.ftz.f32 	%f92, %f91, %f90, %f89;
	add.s32 	%r47, %r52, 28;
	mul.wide.u32 	%rd64, %r47, 4;
	add.s64 	%rd65, %rd1, %rd64;
	ld.shared.f32 	%f93, [_ZZ13matvec_kernelE8x_shared+112];
	ld.global.nc.f32 	%f94, [%rd65];
	fma.rn.ftz.f32 	%f95, %f94, %f93, %f92;
	add.s32 	%r48, %r52, 29;
	mul.wide.u32 	%rd66, %r48, 4;
	add.s64 	%rd67, %rd1, %rd66;
	ld.shared.f32 	%f96, [_ZZ13matvec_kernelE8x_shared+116];
	ld.global.nc.f32 	%f97, [%rd67];
	fma.rn.ftz.f32 	%f98, %f97, %f96, %f95;
	add.s32 	%r49, %r52, 30;
	mul.wide.u32 	%rd68, %r49, 4;
	add.s64 	%rd69, %rd1, %rd68;
	ld.shared.f32 	%f99, [_ZZ13matvec_kernelE8x_shared+120];
	ld.global.nc.f32 	%f100, [%rd69];
	fma.rn.ftz.f32 	%f101, %f100, %f99, %f98;
	add.s32 	%r50, %r52, 31;
	mul.wide.u32 	%rd70, %r50, 4;
	add.s64 	%rd71, %rd1, %rd70;
	ld.shared.f32 	%f102, [_ZZ13matvec_kernelE8x_shared+124];
	ld.global.nc.f32 	%f103, [%rd71];
	fma.rn.ftz.f32 	%f106, %f103, %f102, %f101;
	bar.sync 	0;
	add.s32 	%r52, %r52, 32;
	add.s32 	%r51, %r51, 32;
	add.s32 	%r53, %r53, 1;
	setp.ne.s32 	%p3, %r53, 0;
	@%p3 bra 	$L__BB70_2;

$L__BB70_5:
	setp.ge.u32 	%p4, %r2, %r13;
	@%p4 bra 	$L__BB70_7;

	cvta.to.global.u64 	%rd72, %rd5;
	mul.wide.u32 	%rd73, %r2, 4;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.f32 	[%rd74], %f106;

$L__BB70_7:
	ret;

}
	// .globl	matvec_kernel_half
.visible .entry matvec_kernel_half(
	.param .u64 matvec_kernel_half_param_0,
	.param .u64 matvec_kernel_half_param_1,
	.param .u64 matvec_kernel_half_param_2,
	.param .u32 matvec_kernel_half_param_3,
	.param .u32 matvec_kernel_half_param_4
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<205>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<54>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<75>;
	// demoted variable
	.shared .align 2 .b8 _ZZ18matvec_kernel_halfE8x_shared[64];

	ld.param.u64 	%rd3, [matvec_kernel_half_param_0];
	ld.param.u64 	%rd4, [matvec_kernel_half_param_1];
	ld.param.u64 	%rd5, [matvec_kernel_half_param_2];
	ld.param.u32 	%r13, [matvec_kernel_half_param_3];
	ld.param.u32 	%r14, [matvec_kernel_half_param_4];
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r51, %tid.x;
	mad.lo.s32 	%r2, %r16, %r15, %r51;
	mov.f64 	%fd1, 0d0000000000000000;
	// begin inline asm
	{  cvt.rn.f16.f64 %rs204, %fd1;}

	// end inline asm
	add.s32 	%r17, %r14, 31;
	shr.u32 	%r3, %r17, 5;
	setp.eq.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB71_6;

	cvta.to.global.u64 	%rd1, %rd3;
	shl.b32 	%r18, %r51, 1;
	mov.u32 	%r19, _ZZ18matvec_kernel_halfE8x_shared;
	add.s32 	%r4, %r19, %r18;
	mul.lo.s32 	%r52, %r2, %r14;
	neg.s32 	%r53, %r3;
	cvta.to.global.u64 	%rd2, %rd4;

$L__BB71_2:
	setp.lt.u32 	%p2, %r51, %r14;
	@%p2 bra 	$L__BB71_4;
	bra.uni 	$L__BB71_3;

$L__BB71_4:
	mul.wide.u32 	%rd6, %r51, 2;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.nc.u16 	%rs203, [%rd7];
	bra.uni 	$L__BB71_5;

$L__BB71_3:
	mov.f32 	%f1, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs203, %f1;}

	// end inline asm

$L__BB71_5:
	st.shared.u16 	[%r4], %rs203;
	bar.sync 	0;
	mul.wide.u32 	%rd8, %r52, 2;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.nc.u16 	%rs11, [%rd9];
	ld.shared.u16 	%rs12, [_ZZ18matvec_kernel_halfE8x_shared];
	// begin inline asm
	{mul.f16 %rs10,%rs11,%rs12;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs13,%rs204,%rs10;
}
	// end inline asm
	add.s32 	%r20, %r52, 1;
	mul.wide.u32 	%rd10, %r20, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.u16 	%rs17, [%rd11];
	ld.shared.u16 	%rs18, [_ZZ18matvec_kernel_halfE8x_shared+2];
	// begin inline asm
	{mul.f16 %rs16,%rs17,%rs18;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs19,%rs13,%rs16;
}
	// end inline asm
	add.s32 	%r21, %r52, 2;
	mul.wide.u32 	%rd12, %r21, 2;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.u16 	%rs23, [%rd13];
	ld.shared.u16 	%rs24, [_ZZ18matvec_kernel_halfE8x_shared+4];
	// begin inline asm
	{mul.f16 %rs22,%rs23,%rs24;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs25,%rs19,%rs22;
}
	// end inline asm
	add.s32 	%r22, %r52, 3;
	mul.wide.u32 	%rd14, %r22, 2;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.u16 	%rs29, [%rd15];
	ld.shared.u16 	%rs30, [_ZZ18matvec_kernel_halfE8x_shared+6];
	// begin inline asm
	{mul.f16 %rs28,%rs29,%rs30;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs31,%rs25,%rs28;
}
	// end inline asm
	add.s32 	%r23, %r52, 4;
	mul.wide.u32 	%rd16, %r23, 2;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.u16 	%rs35, [%rd17];
	ld.shared.u16 	%rs36, [_ZZ18matvec_kernel_halfE8x_shared+8];
	// begin inline asm
	{mul.f16 %rs34,%rs35,%rs36;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs37,%rs31,%rs34;
}
	// end inline asm
	add.s32 	%r24, %r52, 5;
	mul.wide.u32 	%rd18, %r24, 2;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.nc.u16 	%rs41, [%rd19];
	ld.shared.u16 	%rs42, [_ZZ18matvec_kernel_halfE8x_shared+10];
	// begin inline asm
	{mul.f16 %rs40,%rs41,%rs42;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs43,%rs37,%rs40;
}
	// end inline asm
	add.s32 	%r25, %r52, 6;
	mul.wide.u32 	%rd20, %r25, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.u16 	%rs47, [%rd21];
	ld.shared.u16 	%rs48, [_ZZ18matvec_kernel_halfE8x_shared+12];
	// begin inline asm
	{mul.f16 %rs46,%rs47,%rs48;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs49,%rs43,%rs46;
}
	// end inline asm
	add.s32 	%r26, %r52, 7;
	mul.wide.u32 	%rd22, %r26, 2;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.u16 	%rs53, [%rd23];
	ld.shared.u16 	%rs54, [_ZZ18matvec_kernel_halfE8x_shared+14];
	// begin inline asm
	{mul.f16 %rs52,%rs53,%rs54;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs55,%rs49,%rs52;
}
	// end inline asm
	add.s32 	%r27, %r52, 8;
	mul.wide.u32 	%rd24, %r27, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.u16 	%rs59, [%rd25];
	ld.shared.u16 	%rs60, [_ZZ18matvec_kernel_halfE8x_shared+16];
	// begin inline asm
	{mul.f16 %rs58,%rs59,%rs60;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs61,%rs55,%rs58;
}
	// end inline asm
	add.s32 	%r28, %r52, 9;
	mul.wide.u32 	%rd26, %r28, 2;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.u16 	%rs65, [%rd27];
	ld.shared.u16 	%rs66, [_ZZ18matvec_kernel_halfE8x_shared+18];
	// begin inline asm
	{mul.f16 %rs64,%rs65,%rs66;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs67,%rs61,%rs64;
}
	// end inline asm
	add.s32 	%r29, %r52, 10;
	mul.wide.u32 	%rd28, %r29, 2;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.u16 	%rs71, [%rd29];
	ld.shared.u16 	%rs72, [_ZZ18matvec_kernel_halfE8x_shared+20];
	// begin inline asm
	{mul.f16 %rs70,%rs71,%rs72;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs73,%rs67,%rs70;
}
	// end inline asm
	add.s32 	%r30, %r52, 11;
	mul.wide.u32 	%rd30, %r30, 2;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.nc.u16 	%rs77, [%rd31];
	ld.shared.u16 	%rs78, [_ZZ18matvec_kernel_halfE8x_shared+22];
	// begin inline asm
	{mul.f16 %rs76,%rs77,%rs78;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs79,%rs73,%rs76;
}
	// end inline asm
	add.s32 	%r31, %r52, 12;
	mul.wide.u32 	%rd32, %r31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.u16 	%rs83, [%rd33];
	ld.shared.u16 	%rs84, [_ZZ18matvec_kernel_halfE8x_shared+24];
	// begin inline asm
	{mul.f16 %rs82,%rs83,%rs84;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs85,%rs79,%rs82;
}
	// end inline asm
	add.s32 	%r32, %r52, 13;
	mul.wide.u32 	%rd34, %r32, 2;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.nc.u16 	%rs89, [%rd35];
	ld.shared.u16 	%rs90, [_ZZ18matvec_kernel_halfE8x_shared+26];
	// begin inline asm
	{mul.f16 %rs88,%rs89,%rs90;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs91,%rs85,%rs88;
}
	// end inline asm
	add.s32 	%r33, %r52, 14;
	mul.wide.u32 	%rd36, %r33, 2;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.nc.u16 	%rs95, [%rd37];
	ld.shared.u16 	%rs96, [_ZZ18matvec_kernel_halfE8x_shared+28];
	// begin inline asm
	{mul.f16 %rs94,%rs95,%rs96;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs97,%rs91,%rs94;
}
	// end inline asm
	add.s32 	%r34, %r52, 15;
	mul.wide.u32 	%rd38, %r34, 2;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.nc.u16 	%rs101, [%rd39];
	ld.shared.u16 	%rs102, [_ZZ18matvec_kernel_halfE8x_shared+30];
	// begin inline asm
	{mul.f16 %rs100,%rs101,%rs102;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs103,%rs97,%rs100;
}
	// end inline asm
	add.s32 	%r35, %r52, 16;
	mul.wide.u32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.nc.u16 	%rs107, [%rd41];
	ld.shared.u16 	%rs108, [_ZZ18matvec_kernel_halfE8x_shared+32];
	// begin inline asm
	{mul.f16 %rs106,%rs107,%rs108;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs109,%rs103,%rs106;
}
	// end inline asm
	add.s32 	%r36, %r52, 17;
	mul.wide.u32 	%rd42, %r36, 2;
	add.s64 	%rd43, %rd1, %rd42;
	ld.global.nc.u16 	%rs113, [%rd43];
	ld.shared.u16 	%rs114, [_ZZ18matvec_kernel_halfE8x_shared+34];
	// begin inline asm
	{mul.f16 %rs112,%rs113,%rs114;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs115,%rs109,%rs112;
}
	// end inline asm
	add.s32 	%r37, %r52, 18;
	mul.wide.u32 	%rd44, %r37, 2;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.u16 	%rs119, [%rd45];
	ld.shared.u16 	%rs120, [_ZZ18matvec_kernel_halfE8x_shared+36];
	// begin inline asm
	{mul.f16 %rs118,%rs119,%rs120;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs121,%rs115,%rs118;
}
	// end inline asm
	add.s32 	%r38, %r52, 19;
	mul.wide.u32 	%rd46, %r38, 2;
	add.s64 	%rd47, %rd1, %rd46;
	ld.global.nc.u16 	%rs125, [%rd47];
	ld.shared.u16 	%rs126, [_ZZ18matvec_kernel_halfE8x_shared+38];
	// begin inline asm
	{mul.f16 %rs124,%rs125,%rs126;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs127,%rs121,%rs124;
}
	// end inline asm
	add.s32 	%r39, %r52, 20;
	mul.wide.u32 	%rd48, %r39, 2;
	add.s64 	%rd49, %rd1, %rd48;
	ld.global.nc.u16 	%rs131, [%rd49];
	ld.shared.u16 	%rs132, [_ZZ18matvec_kernel_halfE8x_shared+40];
	// begin inline asm
	{mul.f16 %rs130,%rs131,%rs132;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs133,%rs127,%rs130;
}
	// end inline asm
	add.s32 	%r40, %r52, 21;
	mul.wide.u32 	%rd50, %r40, 2;
	add.s64 	%rd51, %rd1, %rd50;
	ld.global.nc.u16 	%rs137, [%rd51];
	ld.shared.u16 	%rs138, [_ZZ18matvec_kernel_halfE8x_shared+42];
	// begin inline asm
	{mul.f16 %rs136,%rs137,%rs138;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs139,%rs133,%rs136;
}
	// end inline asm
	add.s32 	%r41, %r52, 22;
	mul.wide.u32 	%rd52, %r41, 2;
	add.s64 	%rd53, %rd1, %rd52;
	ld.global.nc.u16 	%rs143, [%rd53];
	ld.shared.u16 	%rs144, [_ZZ18matvec_kernel_halfE8x_shared+44];
	// begin inline asm
	{mul.f16 %rs142,%rs143,%rs144;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs145,%rs139,%rs142;
}
	// end inline asm
	add.s32 	%r42, %r52, 23;
	mul.wide.u32 	%rd54, %r42, 2;
	add.s64 	%rd55, %rd1, %rd54;
	ld.global.nc.u16 	%rs149, [%rd55];
	ld.shared.u16 	%rs150, [_ZZ18matvec_kernel_halfE8x_shared+46];
	// begin inline asm
	{mul.f16 %rs148,%rs149,%rs150;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs151,%rs145,%rs148;
}
	// end inline asm
	add.s32 	%r43, %r52, 24;
	mul.wide.u32 	%rd56, %r43, 2;
	add.s64 	%rd57, %rd1, %rd56;
	ld.global.nc.u16 	%rs155, [%rd57];
	ld.shared.u16 	%rs156, [_ZZ18matvec_kernel_halfE8x_shared+48];
	// begin inline asm
	{mul.f16 %rs154,%rs155,%rs156;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs157,%rs151,%rs154;
}
	// end inline asm
	add.s32 	%r44, %r52, 25;
	mul.wide.u32 	%rd58, %r44, 2;
	add.s64 	%rd59, %rd1, %rd58;
	ld.global.nc.u16 	%rs161, [%rd59];
	ld.shared.u16 	%rs162, [_ZZ18matvec_kernel_halfE8x_shared+50];
	// begin inline asm
	{mul.f16 %rs160,%rs161,%rs162;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs163,%rs157,%rs160;
}
	// end inline asm
	add.s32 	%r45, %r52, 26;
	mul.wide.u32 	%rd60, %r45, 2;
	add.s64 	%rd61, %rd1, %rd60;
	ld.global.nc.u16 	%rs167, [%rd61];
	ld.shared.u16 	%rs168, [_ZZ18matvec_kernel_halfE8x_shared+52];
	// begin inline asm
	{mul.f16 %rs166,%rs167,%rs168;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs169,%rs163,%rs166;
}
	// end inline asm
	add.s32 	%r46, %r52, 27;
	mul.wide.u32 	%rd62, %r46, 2;
	add.s64 	%rd63, %rd1, %rd62;
	ld.global.nc.u16 	%rs173, [%rd63];
	ld.shared.u16 	%rs174, [_ZZ18matvec_kernel_halfE8x_shared+54];
	// begin inline asm
	{mul.f16 %rs172,%rs173,%rs174;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs175,%rs169,%rs172;
}
	// end inline asm
	add.s32 	%r47, %r52, 28;
	mul.wide.u32 	%rd64, %r47, 2;
	add.s64 	%rd65, %rd1, %rd64;
	ld.global.nc.u16 	%rs179, [%rd65];
	ld.shared.u16 	%rs180, [_ZZ18matvec_kernel_halfE8x_shared+56];
	// begin inline asm
	{mul.f16 %rs178,%rs179,%rs180;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs181,%rs175,%rs178;
}
	// end inline asm
	add.s32 	%r48, %r52, 29;
	mul.wide.u32 	%rd66, %r48, 2;
	add.s64 	%rd67, %rd1, %rd66;
	ld.global.nc.u16 	%rs185, [%rd67];
	ld.shared.u16 	%rs186, [_ZZ18matvec_kernel_halfE8x_shared+58];
	// begin inline asm
	{mul.f16 %rs184,%rs185,%rs186;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs187,%rs181,%rs184;
}
	// end inline asm
	add.s32 	%r49, %r52, 30;
	mul.wide.u32 	%rd68, %r49, 2;
	add.s64 	%rd69, %rd1, %rd68;
	ld.global.nc.u16 	%rs191, [%rd69];
	ld.shared.u16 	%rs192, [_ZZ18matvec_kernel_halfE8x_shared+60];
	// begin inline asm
	{mul.f16 %rs190,%rs191,%rs192;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs193,%rs187,%rs190;
}
	// end inline asm
	add.s32 	%r50, %r52, 31;
	mul.wide.u32 	%rd70, %r50, 2;
	add.s64 	%rd71, %rd1, %rd70;
	ld.global.nc.u16 	%rs197, [%rd71];
	ld.shared.u16 	%rs198, [_ZZ18matvec_kernel_halfE8x_shared+62];
	// begin inline asm
	{mul.f16 %rs196,%rs197,%rs198;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs204,%rs193,%rs196;
}
	// end inline asm
	bar.sync 	0;
	add.s32 	%r52, %r52, 32;
	add.s32 	%r51, %r51, 32;
	add.s32 	%r53, %r53, 1;
	setp.ne.s32 	%p3, %r53, 0;
	@%p3 bra 	$L__BB71_2;

$L__BB71_6:
	setp.ge.u32 	%p4, %r2, %r13;
	@%p4 bra 	$L__BB71_8;

	cvta.to.global.u64 	%rd72, %rd5;
	mul.wide.u32 	%rd73, %r2, 2;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.u16 	[%rd74], %rs204;

$L__BB71_8:
	ret;

}
	// .globl	Softmax
.visible .entry Softmax(
	.param .u64 Softmax_param_0,
	.param .u64 Softmax_param_1,
	.param .u32 Softmax_param_2,
	.param .u32 Softmax_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<10>;
	// demoted variable
	.shared .align 4 .b8 _ZZ7SoftmaxE3max[2048];
	// demoted variable
	.shared .align 4 .b8 _ZZ7SoftmaxE3sum[2048];

	ld.param.u64 	%rd3, [Softmax_param_0];
	ld.param.u64 	%rd4, [Softmax_param_1];
	ld.param.u32 	%r6, [Softmax_param_2];
	ld.param.u32 	%r5, [Softmax_param_3];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB72_6;

	cvta.to.global.u64 	%rd5, %rd3;
	mad.lo.s32 	%r13, %r1, %r5, %r2;
	cvt.s64.s32 	%rd1, %r13;
	mul.wide.s32 	%rd6, %r13, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.f32 	%f1, [%rd7];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, _ZZ7SoftmaxE3max;
	add.s32 	%r3, %r15, %r14;
	st.shared.f32 	[%r3], %f1;
	mov.u32 	%r16, _ZZ7SoftmaxE3sum;
	add.s32 	%r4, %r16, %r14;
	mov.u32 	%r17, 0;
	st.shared.u32 	[%r4], %r17;
	bar.sync 	0;
	ld.shared.f32 	%f4, [%r3];
	setp.geu.ftz.f32 	%p4, %f4, %f1;
	@%p4 bra 	$L__BB72_3;

	st.shared.f32 	[%r3], %f1;

$L__BB72_3:
	bar.sync 	0;
	ld.shared.f32 	%f5, [%r3];
	sub.ftz.f32 	%f6, %f1, %f5;
	mul.ftz.f32 	%f7, %f6, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f8, %f7;
	atom.shared.add.f32 	%f9, [%r4], %f8;
	cvta.to.global.u64 	%rd8, %rd4;
	shl.b64 	%rd9, %rd1, 2;
	add.s64 	%rd2, %rd8, %rd9;
	st.global.f32 	[%rd2], %f8;
	bar.sync 	0;
	ld.shared.f32 	%f2, [%r4];
	setp.neu.ftz.f32 	%p5, %f2, 0f00000000;
	ld.global.f32 	%f3, [%rd2];
	@%p5 bra 	$L__BB72_5;
	bra.uni 	$L__BB72_4;

$L__BB72_5:
	div.approx.ftz.f32 	%f12, %f3, %f2;
	st.global.f32 	[%rd2], %f12;
	bra.uni 	$L__BB72_6;

$L__BB72_4:
	mov.f32 	%f10, 0f33D6BF95;
	div.approx.ftz.f32 	%f11, %f3, %f10;
	st.global.f32 	[%rd2], %f11;

$L__BB72_6:
	ret;

}
	// .globl	Softmax_half
.visible .entry Softmax_half(
	.param .u64 Softmax_half_param_0,
	.param .u64 Softmax_half_param_1,
	.param .u32 Softmax_half_param_2,
	.param .u32 Softmax_half_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<54>;
	.reg .f32 	%f<31>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<13>;
	// demoted variable
	.shared .align 2 .b8 _ZZ12Softmax_halfE3max[1024];
	// demoted variable
	.shared .align 2 .b8 _ZZ12Softmax_halfE3sum[1024];

	ld.param.u64 	%rd3, [Softmax_half_param_0];
	ld.param.u64 	%rd4, [Softmax_half_param_1];
	ld.param.u32 	%r6, [Softmax_half_param_2];
	ld.param.u32 	%r5, [Softmax_half_param_3];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %tid.y;
	mad.lo.s32 	%r2, %r11, %r10, %r12;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB73_12;

	cvta.to.global.u64 	%rd5, %rd3;
	mad.lo.s32 	%r13, %r1, %r5, %r2;
	cvt.s64.s32 	%rd1, %r13;
	mul.wide.s32 	%rd6, %r13, 2;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.nc.u16 	%rs1, [%rd7];
	shl.b32 	%r14, %r1, 1;
	mov.u32 	%r15, _ZZ12Softmax_halfE3max;
	add.s32 	%r3, %r15, %r14;
	st.shared.u16 	[%r3], %rs1;
	ld.const.u16 	%rs2, [sh];
	mov.u32 	%r16, _ZZ12Softmax_halfE3sum;
	add.s32 	%r4, %r16, %r14;
	st.shared.u16 	[%r4], %rs2;
	bar.sync 	0;
	ld.shared.u16 	%rs13, [%r3];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs13, %rs1;
  selp.u16 %rs12, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs12, 0;
	@%p4 bra 	$L__BB73_3;

	st.shared.u16 	[%r3], %rs1;

$L__BB73_3:
	bar.sync 	0;
	ld.shared.u16 	%rs17, [%r3];
	// begin inline asm
	{sub.f16 %rs15,%rs1,%rs17;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs15;}

	// end inline asm
	mul.ftz.f32 	%f11, %f9, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f10, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f10;}

	// end inline asm
	mul.wide.s32 	%rd9, %r1, 2;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r16;
	  cvta.shared.u64 	%rd10, %tmp; }
	add.s64 	%rd8, %rd10, %rd9;
	// begin inline asm
	{ atom.add.noftz.f16 %rs20,[%rd8],%rs19; }

	// end inline asm
	cvta.to.global.u64 	%rd11, %rd4;
	shl.b64 	%rd12, %rd1, 1;
	add.s64 	%rd2, %rd11, %rd12;
	st.global.u16 	[%rd2], %rs19;
	bar.sync 	0;
	ld.shared.u16 	%rs3, [%r4];
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.neu.f16  __$temp3, %rs3, %rs2;
  selp.u16 %rs22, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p5, %rs22, 0;
	@%p5 bra 	$L__BB73_8;

	ld.global.u16 	%rs25, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs3;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f14, %f13;
}
	// end inline asm
	mul.ftz.f32 	%f16, %f12, %f14;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs52, %f16;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs28,%rs52;
}
	// end inline asm
	mov.u16 	%rs32, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs28, %rs32;
  selp.u16 %rs30, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs30, 0;
	@%p6 bra 	$L__BB73_7;

	mov.f32 	%f17, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f17;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs33, %rs28;
  selp.u16 %rs34, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs34, 0;
	@%p7 bra 	$L__BB73_7;

	neg.ftz.f32 	%f19, %f13;
	fma.rn.ftz.f32 	%f20, %f19, %f16, %f12;
	fma.rn.ftz.f32 	%f18, %f14, %f20, %f16;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs52, %f18;}

	// end inline asm

$L__BB73_7:
	st.global.u16 	[%rd2], %rs52;
	bra.uni 	$L__BB73_12;

$L__BB73_8:
	mov.f32 	%f21, 0f33D6BF95;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs38, %f21;}

	// end inline asm
	ld.global.u16 	%rs39, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs39;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs38;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f24, %f23;
}
	// end inline asm
	mul.ftz.f32 	%f26, %f22, %f24;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f26;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs42,%rs53;
}
	// end inline asm
	mov.u16 	%rs46, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs42, %rs46;
  selp.u16 %rs44, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p8, %rs44, 0;
	@%p8 bra 	$L__BB73_11;

	mov.f32 	%f27, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs47, %f27;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs47, %rs42;
  selp.u16 %rs48, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p9, %rs48, 0;
	@%p9 bra 	$L__BB73_11;

	neg.ftz.f32 	%f29, %f23;
	fma.rn.ftz.f32 	%f30, %f29, %f26, %f22;
	fma.rn.ftz.f32 	%f28, %f24, %f30, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f28;}

	// end inline asm

$L__BB73_11:
	st.global.u16 	[%rd2], %rs53;

$L__BB73_12:
	ret;

}
	// .globl	derSoftmax
.visible .entry derSoftmax(
	.param .u64 derSoftmax_param_0,
	.param .u64 derSoftmax_param_1,
	.param .u64 derSoftmax_param_2,
	.param .u32 derSoftmax_param_3,
	.param .u32 derSoftmax_param_4
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<101>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<59>;


	ld.param.u64 	%rd35, [derSoftmax_param_0];
	ld.param.u64 	%rd36, [derSoftmax_param_1];
	ld.param.u64 	%rd34, [derSoftmax_param_2];
	ld.param.u32 	%r42, [derSoftmax_param_3];
	ld.param.u32 	%r41, [derSoftmax_param_4];
	cvta.to.global.u64 	%rd1, %rd36;
	cvta.to.global.u64 	%rd2, %rd35;
	mov.u32 	%r43, %ctaid.x;
	mov.u32 	%r44, %ntid.x;
	mov.u32 	%r45, %tid.x;
	mad.lo.s32 	%r1, %r44, %r43, %r45;
	mov.u32 	%r46, %ctaid.y;
	mov.u32 	%r47, %ntid.y;
	mul.lo.s32 	%r2, %r47, %r46;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r42;
	setp.ge.s32 	%p2, %r4, %r41;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB74_23;

	cvta.to.global.u64 	%rd37, %rd34;
	mul.lo.s32 	%r5, %r1, %r41;
	add.s32 	%r49, %r5, %r4;
	mul.wide.s32 	%rd38, %r49, 4;
	add.s64 	%rd3, %rd37, %rd38;
	mov.f32 	%f95, 0f00000000;
	mov.u32 	%r89, 0;
	st.global.u32 	[%rd3], %r89;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.f32 	%f1, [%rd39];
	max.s32 	%r6, %r4, 0;
	min.s32 	%r7, %r6, %r41;
	setp.lt.s32 	%p4, %r7, 1;
	@%p4 bra 	$L__BB74_8;

	add.s32 	%r52, %r7, -1;
	and.b32  	%r82, %r7, 3;
	setp.lt.u32 	%p5, %r52, 3;
	mov.f32 	%f95, 0f00000000;
	mov.u32 	%r89, 0;
	@%p5 bra 	$L__BB74_5;

	not.b32 	%r54, %r6;
	not.b32 	%r55, %r41;
	max.s32 	%r56, %r54, %r55;
	add.s32 	%r57, %r56, %r82;
	neg.s32 	%r77, %r57;
	mul.wide.s32 	%rd40, %r5, 4;
	add.s64 	%rd41, %rd40, 8;
	add.s64 	%rd50, %rd2, %rd41;
	add.s64 	%rd49, %rd1, %rd41;

$L__BB74_4:
	ld.global.nc.f32 	%f28, [%rd50+-8];
	mul.ftz.f32 	%f29, %f28, %f1;
	ld.global.nc.f32 	%f30, [%rd49+-8];
	mul.ftz.f32 	%f31, %f29, %f30;
	sub.ftz.f32 	%f32, %f95, %f31;
	ld.global.nc.f32 	%f33, [%rd50+-4];
	mul.ftz.f32 	%f34, %f33, %f1;
	ld.global.nc.f32 	%f35, [%rd49+-4];
	mul.ftz.f32 	%f36, %f34, %f35;
	sub.ftz.f32 	%f37, %f32, %f36;
	ld.global.nc.f32 	%f38, [%rd50];
	mul.ftz.f32 	%f39, %f38, %f1;
	ld.global.nc.f32 	%f40, [%rd49];
	mul.ftz.f32 	%f41, %f39, %f40;
	sub.ftz.f32 	%f42, %f37, %f41;
	ld.global.nc.f32 	%f43, [%rd50+4];
	mul.ftz.f32 	%f44, %f43, %f1;
	ld.global.nc.f32 	%f45, [%rd49+4];
	mul.ftz.f32 	%f46, %f44, %f45;
	sub.ftz.f32 	%f95, %f42, %f46;
	add.s32 	%r89, %r89, 4;
	add.s64 	%rd50, %rd50, 16;
	add.s64 	%rd49, %rd49, 16;
	add.s32 	%r77, %r77, -4;
	setp.ne.s32 	%p6, %r77, 1;
	@%p6 bra 	$L__BB74_4;

$L__BB74_5:
	setp.eq.s32 	%p7, %r82, 0;
	@%p7 bra 	$L__BB74_8;

	add.s32 	%r58, %r89, %r5;
	mul.wide.s32 	%rd42, %r58, 4;
	add.s64 	%rd52, %rd1, %rd42;
	add.s64 	%rd51, %rd2, %rd42;

$L__BB74_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f47, [%rd51];
	mul.ftz.f32 	%f48, %f47, %f1;
	ld.global.nc.f32 	%f49, [%rd52];
	mul.ftz.f32 	%f50, %f48, %f49;
	sub.ftz.f32 	%f95, %f95, %f50;
	add.s32 	%r89, %r89, 1;
	add.s64 	%rd52, %rd52, 4;
	add.s64 	%rd51, %rd51, 4;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p8, %r82, 0;
	@%p8 bra 	$L__BB74_7;

$L__BB74_8:
	add.s32 	%r59, %r4, 1;
	min.s32 	%r21, %r59, %r41;
	setp.ge.s32 	%p9, %r89, %r21;
	@%p9 bra 	$L__BB74_15;

	mov.f32 	%f52, 0f3F800000;
	sub.ftz.f32 	%f53, %f52, %f1;
	mul.ftz.f32 	%f9, %f1, %f53;
	mov.u32 	%r61, -2;
	sub.s32 	%r62, %r61, %r2;
	sub.s32 	%r63, %r62, %r3;
	not.b32 	%r64, %r41;
	max.s32 	%r22, %r63, %r64;
	not.b32 	%r65, %r89;
	sub.s32 	%r66, %r65, %r22;
	and.b32  	%r85, %r66, 3;
	setp.eq.s32 	%p10, %r85, 0;
	mov.u32 	%r86, %r89;
	@%p10 bra 	$L__BB74_12;

	add.s32 	%r67, %r89, %r5;
	mul.wide.s32 	%rd43, %r67, 4;
	add.s64 	%rd53, %rd1, %rd43;
	mov.u32 	%r86, %r89;

$L__BB74_11:
	.pragma "nounroll";
	ld.global.nc.f32 	%f54, [%rd53];
	fma.rn.ftz.f32 	%f95, %f54, %f9, %f95;
	add.s32 	%r86, %r86, 1;
	add.s64 	%rd53, %rd53, 4;
	add.s32 	%r85, %r85, -1;
	setp.ne.s32 	%p11, %r85, 0;
	@%p11 bra 	$L__BB74_11;

$L__BB74_12:
	sub.s32 	%r69, %r61, %r89;
	sub.s32 	%r70, %r69, %r22;
	setp.lt.u32 	%p12, %r70, 3;
	mov.u32 	%r89, %r86;
	@%p12 bra 	$L__BB74_15;

	add.s32 	%r71, %r86, %r5;
	mul.wide.s32 	%rd44, %r71, 4;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd54, %rd45, 8;
	mov.u32 	%r89, %r86;

$L__BB74_14:
	ld.global.nc.f32 	%f55, [%rd54+-8];
	fma.rn.ftz.f32 	%f56, %f55, %f9, %f95;
	ld.global.nc.f32 	%f57, [%rd54+-4];
	fma.rn.ftz.f32 	%f58, %f57, %f9, %f56;
	ld.global.nc.f32 	%f59, [%rd54];
	fma.rn.ftz.f32 	%f60, %f59, %f9, %f58;
	ld.global.nc.f32 	%f61, [%rd54+4];
	fma.rn.ftz.f32 	%f95, %f61, %f9, %f60;
	add.s64 	%rd54, %rd54, 16;
	add.s32 	%r89, %r89, 4;
	setp.lt.s32 	%p13, %r89, %r21;
	@%p13 bra 	$L__BB74_14;

$L__BB74_15:
	setp.ge.s32 	%p14, %r89, %r41;
	@%p14 bra 	$L__BB74_22;

	sub.s32 	%r72, %r41, %r89;
	and.b32  	%r91, %r72, 3;
	setp.eq.s32 	%p15, %r91, 0;
	mov.u32 	%r92, %r89;
	@%p15 bra 	$L__BB74_19;

	add.s32 	%r73, %r89, %r5;
	mul.wide.s32 	%rd46, %r73, 4;
	add.s64 	%rd56, %rd1, %rd46;
	add.s64 	%rd55, %rd2, %rd46;
	mov.u32 	%r92, %r89;

$L__BB74_18:
	.pragma "nounroll";
	ld.global.nc.f32 	%f63, [%rd55];
	mul.ftz.f32 	%f64, %f63, %f1;
	ld.global.nc.f32 	%f65, [%rd56];
	mul.ftz.f32 	%f66, %f64, %f65;
	sub.ftz.f32 	%f95, %f95, %f66;
	add.s32 	%r92, %r92, 1;
	add.s64 	%rd56, %rd56, 4;
	add.s64 	%rd55, %rd55, 4;
	add.s32 	%r91, %r91, -1;
	setp.ne.s32 	%p16, %r91, 0;
	@%p16 bra 	$L__BB74_18;

$L__BB74_19:
	not.b32 	%r74, %r89;
	add.s32 	%r75, %r74, %r41;
	setp.lt.u32 	%p17, %r75, 3;
	@%p17 bra 	$L__BB74_22;

	add.s32 	%r76, %r92, %r5;
	mul.wide.s32 	%rd47, %r76, 4;
	add.s64 	%rd48, %rd47, 8;
	add.s64 	%rd58, %rd1, %rd48;
	add.s64 	%rd57, %rd2, %rd48;

$L__BB74_21:
	ld.global.nc.f32 	%f67, [%rd57+-8];
	mul.ftz.f32 	%f68, %f67, %f1;
	ld.global.nc.f32 	%f69, [%rd58+-8];
	mul.ftz.f32 	%f70, %f68, %f69;
	sub.ftz.f32 	%f71, %f95, %f70;
	ld.global.nc.f32 	%f72, [%rd57+-4];
	mul.ftz.f32 	%f73, %f72, %f1;
	ld.global.nc.f32 	%f74, [%rd58+-4];
	mul.ftz.f32 	%f75, %f73, %f74;
	sub.ftz.f32 	%f76, %f71, %f75;
	ld.global.nc.f32 	%f77, [%rd57];
	mul.ftz.f32 	%f78, %f77, %f1;
	ld.global.nc.f32 	%f79, [%rd58];
	mul.ftz.f32 	%f80, %f78, %f79;
	sub.ftz.f32 	%f81, %f76, %f80;
	ld.global.nc.f32 	%f82, [%rd57+4];
	mul.ftz.f32 	%f83, %f82, %f1;
	ld.global.nc.f32 	%f84, [%rd58+4];
	mul.ftz.f32 	%f85, %f83, %f84;
	sub.ftz.f32 	%f95, %f81, %f85;
	add.s64 	%rd58, %rd58, 16;
	add.s64 	%rd57, %rd57, 16;
	add.s32 	%r92, %r92, 4;
	setp.lt.s32 	%p18, %r92, %r41;
	@%p18 bra 	$L__BB74_21;

$L__BB74_22:
	st.global.f32 	[%rd3], %f95;

$L__BB74_23:
	ret;

}
	// .globl	derSoftmax_half
.visible .entry derSoftmax_half(
	.param .u64 derSoftmax_half_param_0,
	.param .u64 derSoftmax_half_param_1,
	.param .u64 derSoftmax_half_param_2,
	.param .u32 derSoftmax_half_param_3,
	.param .u32 derSoftmax_half_param_4
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<132>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd18, [derSoftmax_half_param_0];
	ld.param.u64 	%rd20, [derSoftmax_half_param_1];
	ld.param.u64 	%rd19, [derSoftmax_half_param_2];
	ld.param.u32 	%r22, [derSoftmax_half_param_3];
	ld.param.u32 	%r21, [derSoftmax_half_param_4];
	cvta.to.global.u64 	%rd1, %rd20;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r1, %r24, %r23, %r25;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %ntid.y;
	mul.lo.s32 	%r2, %r27, %r26;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.ge.s32 	%p1, %r1, %r22;
	setp.ge.s32 	%p2, %r4, %r21;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB75_28;

	cvta.to.global.u64 	%rd21, %rd19;
	ld.const.u16 	%rs130, [sh];
	mul.lo.s32 	%r5, %r1, %r21;
	add.s32 	%r28, %r5, %r4;
	mul.wide.s32 	%rd22, %r28, 2;
	add.s64 	%rd3, %rd21, %rd22;
	st.global.u16 	[%rd3], %rs130;
	add.s64 	%rd23, %rd2, %rd22;
	ld.global.nc.u16 	%rs2, [%rd23];
	setp.lt.s32 	%p4, %r21, 1;
	@%p4 bra 	$L__BB75_23;

	ld.const.u16 	%rs3, [sh+8];
	and.b32  	%r42, %r21, 3;
	add.s32 	%r30, %r21, -1;
	setp.lt.u32 	%p5, %r30, 3;
	mov.u32 	%r40, 0;
	@%p5 bra 	$L__BB75_17;

	sub.s32 	%r39, %r21, %r42;
	neg.s32 	%r37, %r4;
	mul.wide.s32 	%rd24, %r5, 2;
	add.s64 	%rd25, %rd24, 4;
	add.s64 	%rd29, %rd1, %rd25;
	add.s64 	%rd28, %rd2, %rd25;
	// begin inline asm
	{sub.f16 %rs39,%rs3,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs42,%rs2,%rs39;
}
	// end inline asm

$L__BB75_4:
	add.s64 	%rd8, %rd28, -4;
	setp.eq.s32 	%p6, %r37, 0;
	@%p6 bra 	$L__BB75_6;

	ld.global.nc.u16 	%rs35, [%rd8];
	// begin inline asm
	{neg.f16 %rs34,%rs35;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs122,%rs2,%rs34;
}
	// end inline asm
	bra.uni 	$L__BB75_7;

$L__BB75_6:
	mov.u16 	%rs122, %rs42;

$L__BB75_7:
	add.s64 	%rd9, %rd29, -4;
	ld.global.nc.u16 	%rs46, [%rd29+-4];
	// begin inline asm
	{mul.f16 %rs45,%rs46,%rs122;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs48,%rs130,%rs45;
}
	// end inline asm
	add.s32 	%r32, %r40, 1;
	setp.eq.s32 	%p7, %r4, %r32;
	@%p7 bra 	$L__BB75_9;
	bra.uni 	$L__BB75_8;

$L__BB75_9:
	mov.u16 	%rs123, %rs42;
	bra.uni 	$L__BB75_10;

$L__BB75_8:
	ld.global.nc.u16 	%rs52, [%rd8+2];
	// begin inline asm
	{neg.f16 %rs51,%rs52;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs123,%rs2,%rs51;
}
	// end inline asm

$L__BB75_10:
	ld.global.nc.u16 	%rs63, [%rd9+2];
	// begin inline asm
	{mul.f16 %rs62,%rs63,%rs123;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs65,%rs48,%rs62;
}
	// end inline asm
	add.s32 	%r33, %r40, 2;
	setp.eq.s32 	%p8, %r4, %r33;
	@%p8 bra 	$L__BB75_12;
	bra.uni 	$L__BB75_11;

$L__BB75_12:
	mov.u16 	%rs124, %rs42;
	bra.uni 	$L__BB75_13;

$L__BB75_11:
	ld.global.nc.u16 	%rs69, [%rd8+4];
	// begin inline asm
	{neg.f16 %rs68,%rs69;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs124,%rs2,%rs68;
}
	// end inline asm

$L__BB75_13:
	ld.global.nc.u16 	%rs80, [%rd9+4];
	// begin inline asm
	{mul.f16 %rs79,%rs80,%rs124;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs82,%rs65,%rs79;
}
	// end inline asm
	add.s32 	%r34, %r40, 3;
	setp.eq.s32 	%p9, %r4, %r34;
	@%p9 bra 	$L__BB75_15;
	bra.uni 	$L__BB75_14;

$L__BB75_15:
	mov.u16 	%rs125, %rs42;
	bra.uni 	$L__BB75_16;

$L__BB75_14:
	ld.global.nc.u16 	%rs86, [%rd8+6];
	// begin inline asm
	{neg.f16 %rs85,%rs86;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs125,%rs2,%rs85;
}
	// end inline asm

$L__BB75_16:
	ld.global.nc.u16 	%rs97, [%rd9+6];
	// begin inline asm
	{mul.f16 %rs96,%rs97,%rs125;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs130,%rs82,%rs96;
}
	// end inline asm
	add.s32 	%r40, %r40, 4;
	add.s32 	%r37, %r37, 4;
	add.s64 	%rd29, %rd29, 8;
	add.s64 	%rd28, %rd28, 8;
	add.s32 	%r39, %r39, -4;
	setp.ne.s32 	%p10, %r39, 0;
	@%p10 bra 	$L__BB75_4;

$L__BB75_17:
	setp.eq.s32 	%p11, %r42, 0;
	@%p11 bra 	$L__BB75_23;

	sub.s32 	%r35, %r40, %r2;
	sub.s32 	%r41, %r35, %r3;
	add.s32 	%r36, %r40, %r5;
	mul.wide.s32 	%rd26, %r36, 2;
	add.s64 	%rd31, %rd1, %rd26;
	add.s64 	%rd30, %rd2, %rd26;
	// begin inline asm
	{sub.f16 %rs107,%rs3,%rs2;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs110,%rs2,%rs107;
}
	// end inline asm

$L__BB75_19:
	.pragma "nounroll";
	setp.eq.s32 	%p12, %r41, 0;
	@%p12 bra 	$L__BB75_21;

	ld.global.nc.u16 	%rs103, [%rd30];
	// begin inline asm
	{neg.f16 %rs102,%rs103;
}
	// end inline asm
	// begin inline asm
	{mul.f16 %rs129,%rs2,%rs102;
}
	// end inline asm
	bra.uni 	$L__BB75_22;

$L__BB75_21:
	mov.u16 	%rs129, %rs110;

$L__BB75_22:
	ld.global.nc.u16 	%rs114, [%rd31];
	// begin inline asm
	{mul.f16 %rs113,%rs114,%rs129;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs130,%rs130,%rs113;
}
	// end inline asm
	add.s32 	%r41, %r41, 1;
	add.s64 	%rd31, %rd31, 2;
	add.s64 	%rd30, %rd30, 2;
	add.s32 	%r42, %r42, -1;
	setp.ne.s32 	%p13, %r42, 0;
	@%p13 bra 	$L__BB75_19;

$L__BB75_23:
	setp.eq.s16 	%p14, %rs130, 31744;
	@%p14 bra 	$L__BB75_26;
	bra.uni 	$L__BB75_24;

$L__BB75_26:
	ld.const.u16 	%rs130, [sh+24];
	bra.uni 	$L__BB75_27;

$L__BB75_24:
	setp.ne.s16 	%p15, %rs130, -1024;
	@%p15 bra 	$L__BB75_27;

	ld.const.u16 	%rs120, [sh+24];
	// begin inline asm
	{neg.f16 %rs130,%rs120;
}
	// end inline asm

$L__BB75_27:
	st.global.u16 	[%rd3], %rs130;

$L__BB75_28:
	ret;

}

